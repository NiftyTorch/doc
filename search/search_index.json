{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NiftyTorch is a Python API for deploying deep neural networks for Neuroimaging research. Motivation The motivation behind the development of such a library is that there is no centralized tool for deployig 3D deep learning for neuroimaing. In addition, most of the existing tools require expert technical know-how in deep learning or programming, creating a barrier for entry. The goal is to provide a one stop API using which the users can perform classification tasks, Segmentation tasks and Image Generation tasks. The intended audience are the members of neuroimaging who would like to explore deep learning but have no background in coding. Highlighted Features Pytorch Embedded End to End data-loading pipeline Built-in Attention module to demographic data and other modalities or mask Additional Loss Functions (not in PyTorch) incorporated to easily use with any network use Automatic Hyperparameter tuning for each network Easily Customizable to use your own network Multi-scale Training Built-in classification networks Built-in CNNs units such as Bottleneck, BinaryActivation Features Data Loader Classification DataLoader for nifty files Segmentation DataLoader for nifty files Loss Functions Cross Entropy Focal Loss Dice Loss Focal Dice Loss Tversky Loss Lovarsz Softmax Triplet Loss Contrastive Loss Support for including demographic information using attention Position Attention Channel Attention Convolutional Neural Network Units (Layers) BottleNeck Unit Fire Unit Shuffle Unit Binary Activation Binary Convolution Data Augmentation (Transformations) Noise Addition Rotation Random Segmentation Crop Resize Models AlexNet VGGNet ResNet ShuffleNet SqueezeNet XNORNet Training Data level Parallelization Multi Scale Training Hyperparameter Training Installation NiftyTorch can be installed using: pip install niftytorch For a complete demo on how to set up the requirement and getting started see Getting Started notebook in the Demo folder. Resources For Tutorials and Demos , please visit Demo Repository . For Announcements and News , follow us on Twitter @NiftyTorch . For Please submit your questions and suggestions via niftytorch @ gmail.com . We appreciate your constructive inputs. Developers Adithya Subramanian and Farshid Sepehrband INI Microstructural imaging Group ( IMG ) Mark and Mary Stevens Neuroimaging and Informatics Institute ( INI ) Keck School of Medicine of USC Other Contributors Sankareswari Govindarajan and Haoyu Lan INI Microstructural imaging Group ( IMG ) Acknowledgement NiftyTorch would not be possible without liberal imports of the excellent pytorch , torchvision , nipy , numpy , pandas , matplotlib and optuna libraries.","title":"NiftyTorch"},{"location":"#motivation","text":"The motivation behind the development of such a library is that there is no centralized tool for deployig 3D deep learning for neuroimaing. In addition, most of the existing tools require expert technical know-how in deep learning or programming, creating a barrier for entry. The goal is to provide a one stop API using which the users can perform classification tasks, Segmentation tasks and Image Generation tasks. The intended audience are the members of neuroimaging who would like to explore deep learning but have no background in coding.","title":"Motivation"},{"location":"#highlighted-features","text":"Pytorch Embedded End to End data-loading pipeline Built-in Attention module to demographic data and other modalities or mask Additional Loss Functions (not in PyTorch) incorporated to easily use with any network use Automatic Hyperparameter tuning for each network Easily Customizable to use your own network Multi-scale Training Built-in classification networks Built-in CNNs units such as Bottleneck, BinaryActivation","title":"Highlighted Features"},{"location":"#features","text":"Data Loader Classification DataLoader for nifty files Segmentation DataLoader for nifty files Loss Functions Cross Entropy Focal Loss Dice Loss Focal Dice Loss Tversky Loss Lovarsz Softmax Triplet Loss Contrastive Loss Support for including demographic information using attention Position Attention Channel Attention Convolutional Neural Network Units (Layers) BottleNeck Unit Fire Unit Shuffle Unit Binary Activation Binary Convolution Data Augmentation (Transformations) Noise Addition Rotation Random Segmentation Crop Resize Models AlexNet VGGNet ResNet ShuffleNet SqueezeNet XNORNet Training Data level Parallelization Multi Scale Training Hyperparameter Training","title":"Features"},{"location":"#installation","text":"NiftyTorch can be installed using: pip install niftytorch For a complete demo on how to set up the requirement and getting started see Getting Started notebook in the Demo folder.","title":"Installation"},{"location":"#resources","text":"For Tutorials and Demos , please visit Demo Repository . For Announcements and News , follow us on Twitter @NiftyTorch . For Please submit your questions and suggestions via niftytorch @ gmail.com . We appreciate your constructive inputs.","title":"Resources"},{"location":"#developers","text":"Adithya Subramanian and Farshid Sepehrband INI Microstructural imaging Group ( IMG ) Mark and Mary Stevens Neuroimaging and Informatics Institute ( INI ) Keck School of Medicine of USC","title":"Developers"},{"location":"#other-contributors","text":"Sankareswari Govindarajan and Haoyu Lan INI Microstructural imaging Group ( IMG )","title":"Other Contributors"},{"location":"#acknowledgement","text":"NiftyTorch would not be possible without liberal imports of the excellent pytorch , torchvision , nipy , numpy , pandas , matplotlib and optuna libraries.","title":"Acknowledgement"},{"location":"attention/","text":"Attention The Attention can be used in the form of self-attention to identify important positional features or channel features. It can be also used to add demographic data into the model. Modules Positional Attention The code below demonstrates how to use the Positional Attention Module (PAM). The object constructer for the Position Attention Module used to attend to different location specific features via aggreagation context. Parameters for Constructor: in_shape (int,required): the number of channels in the input tensor for PAM Module reduction (int,default = 8): the compression in features channels to be done before computing the attention query_conv_kernel (int, default = 1): The kernel size for convolutional filter applied in query features key_conv_kernel (int, default = 1): The kernel size for convolutional filter applied in key features value_conv_kernel (int, default = 1): The kernel size for convolutional filter applied in value features Usage: from NiftyTorch.Attention.Attention import PAM_Module PAM = PAM_Module(in_shape = 512,reduction = 8,query_conv_kernel = 3,key_conv_kernel = 3,value_conv_kernel = 3) t = torch.rand(64,512,32,32) out,attention = PAM(t) print(out.shape) >>> 64 x 512 x 32 x 32 print(attention.shape) >>> 64 x 1024 x 1024 The PAM returns two tensors where the first tensor is the output from positional attention and the second is the attention map. Channel Attention The code below demonstrates how to use the Channel Attention Module (CAM). The object constructer for the Channel Attention Module used to attend to different channel specific features. Parameters for Constructor: in_shape (int,required): the number of channels in the input tensor for CAM Module Usage: from NiftyTorch.Attention.Attention import CAM_Module PAM = CAM_Module(512) t = torch.rand(64,512,32,32) out,attention = CAM(t) print(out.shape) >>> 64 x 512 x 32 x 32 print(attention.shape) >>> 64 x 512 x 512 The CAM returns two tensors where the first tensor is the output from channel-wise attention and the second is the attention map.","title":"Attention"},{"location":"attention/#attention","text":"The Attention can be used in the form of self-attention to identify important positional features or channel features. It can be also used to add demographic data into the model.","title":"Attention"},{"location":"attention/#modules","text":"","title":"Modules"},{"location":"attention/#positional-attention","text":"The code below demonstrates how to use the Positional Attention Module (PAM). The object constructer for the Position Attention Module used to attend to different location specific features via aggreagation context. Parameters for Constructor: in_shape (int,required): the number of channels in the input tensor for PAM Module reduction (int,default = 8): the compression in features channels to be done before computing the attention query_conv_kernel (int, default = 1): The kernel size for convolutional filter applied in query features key_conv_kernel (int, default = 1): The kernel size for convolutional filter applied in key features value_conv_kernel (int, default = 1): The kernel size for convolutional filter applied in value features Usage: from NiftyTorch.Attention.Attention import PAM_Module PAM = PAM_Module(in_shape = 512,reduction = 8,query_conv_kernel = 3,key_conv_kernel = 3,value_conv_kernel = 3) t = torch.rand(64,512,32,32) out,attention = PAM(t) print(out.shape) >>> 64 x 512 x 32 x 32 print(attention.shape) >>> 64 x 1024 x 1024 The PAM returns two tensors where the first tensor is the output from positional attention and the second is the attention map.","title":"Positional Attention"},{"location":"attention/#channel-attention","text":"The code below demonstrates how to use the Channel Attention Module (CAM). The object constructer for the Channel Attention Module used to attend to different channel specific features. Parameters for Constructor: in_shape (int,required): the number of channels in the input tensor for CAM Module Usage: from NiftyTorch.Attention.Attention import CAM_Module PAM = CAM_Module(512) t = torch.rand(64,512,32,32) out,attention = CAM(t) print(out.shape) >>> 64 x 512 x 32 x 32 print(attention.shape) >>> 64 x 512 x 512 The CAM returns two tensors where the first tensor is the output from channel-wise attention and the second is the attention map.","title":"Channel Attention"},{"location":"layers/","text":"Layers The Layers module contains set of convolutional neural network units, which can used to create custom neural network architectures. Modules Conv3x3 This is a simple conv 3x3 layer. Parameters: in_planes (int,required): The number of channels in the input feature map. out_planes (int,requied): The number of channels in the output feature map. stride (int, default = 1): The stride used in each convolution filter. groups (int, default = 1): The number of groups to be considered while performing convolution. dilation (int, default = 1): The field of view to be considered while performing convolution. bias (int, default = False) Usage: from Layers.Convolutional_Layers import Conv3x3 import torch in_planes = 512 input = torch.rand(32,in_planes,32,32,32) convolution = Conv3x3(in_planes = in_planes,out_planes = 32,stride = 1,groups = 1,dilation = 1,bias = False) output = convolution(input) Conv1x1 This is a simple conv 1x1 layer. Parameters: in_planes (int,required): The number of channels in the input feature map. out_planes (int,requied): The number of channels in the output feature map. stride (int, default = 1): The stride used in each convolution filter. groups (int, default = 1): The number of groups to be considered while performing convolution. bias (int, default = False) Usage: from NiftyTorch.Layers.Convolutional_Layers import Conv1x1 import torch in_planes = 512 input = torch.rand(32,in_planes,32,32,32) convolution = Conv1x1(in_planes = in_planes,out_planes = 32,stride = 1,groups = 1,bias = False) output = convolution(input) Basic Block BasicBlock is a series of convolution, batchnorm, activation layers with one jump skip connection. The structure of this block is as follows: input -> conv1 -> bn1 -> relu -> conv2 -> bn2 + input -> relu : output Parameters: inplanes (int,required): The number of channels in the input feature map. planes (int,required): The number of channels in the output feature map. stride (int,default = 1): The number of strides to be used in the conv1 downsample (int,default = None): The downsampler to be used to reduce the feature map size F.interpolate groups (int,default = 1): The number of groups to be considered in convolution dilation (int, default = 1): The field of view to be considered while performing convolution. norm_layer (int,default = None): The normalization layer to be used ex: nn.BatchNorm() Usage: from NiftyTorch.Layers.Convolutional_Layers import BasicBlock import torch in_planes = 512 input = torch.rand(32,in_planes,32,32,32) convolution_block = BasicBlock(in_planes = in_planes,planes = 256,out_planes = 32,stride = 1,groups = 1,bias = False) output = convolution_block(input) BottleNeck Block BottleNeck is a series of convolution,batchnorm,activation layers with two layer jump skip connection. The structure of this block is as follows: input -> conv1 -> bn1 -> relu -> conv2 -> bn2 -> relu -> conv3 -> bn3 + input -> relu : output Parameters: inplanes (int,required): The number of channels in the input feature map. planes (int,required): The number of channels in the output feature map. stride (int,default = 1): The number of strides to be used in the conv1 downsample (int,default = None): The downsampler to be used to reduce the feature map size F.interpolate groups (int,default = 1): The number of groups to be considered in convolution. base_width (int,default = 1): The base width and expansion are used to calculate the number of filters for conv2 in bottleneck block. dilation (int, default = 1): The field of view to be considered while performing convolution. norm_layer (int,default = None): The normalization layer to be used ex: nn.BatchNorm() expansion (int,default = 1):The base width and expansion are used to calculate the number of filters for conv2 in bottleneck block. Usage: from NiftyTorch.Layers.Convolutional_Layers import BottleNeck import torch in_planes = 512 planes = 256 input = torch.rand(32,in_planes,32,32,32) convolution_block = BottleNeck(inplanes = in_planes, planes = planes, stride=1, downsample=None, groups=1,base_width=64, dilation=1, norm_layer=None,expansion = 1) output = convolution_block(input) ShuffleUnit ShuffleUnit is the class definition for ShuffleUnit Module, which uses channel shuffling and combination to improve the network performance with fewer parameters. The ShuffleUnit is commonly used in ShuffleNet 1.0 and ShuffleNet 2.0, but they can be used along with any network. For more information, please read the ShuffleNet papers [ ShuffleNet 1.0 , ShuffleNet 2.0 ]. Parameters: in_channels (int,required): number of channels in the input tensor the shuffleunit. out_channels (int,required): number of channels in the tensor generated from the shuffleunit. groups (int,default = 3): define number of groups in which the filters are combined before shuffle operation. grouped_conv (bool,default = True): defines whether grouped_convolution is to be used or not. combine ('add' or 'concat',default = 'add'): defines the method in which we can combine the channels it has two options add or concat. compresstion_ratio (int,default = 4): The number of channels to be required in the bottleneck channels. Usage: from NiftyTorch.Layers.Convolutional_Layers import ShuffleUnit import torch in_planes = 512 planes = 256 input = torch.rand(32,in_planes,32,32,32) convolution_block = ShuffleUnit(inplanes = in_planes, planes = planes, groups=5, grouped_conv=True, combine = 'concat',compression_ratio = 4) output = convolution_block(input) Fire Module Fire is the class definition for Fire Module, which uses a combination of 1x1 and 3x3 filters to improve the network performance with fewer parameters. The Fire Module is commonly used in Squeezenet 1.0 and SqueezeNet 2.0 but they can used along with any network. For more information please read the SqueezeNet paper . Parameters: in_channels (int,required): number of channels in the input tensor the shuffleunit. out_channels (int,required): number of channels in the tensor generated from the shuffleunit. groups (int,default = 3): define number of groups in which the filters are combined before shuffle operation. grouped_conv (bool,default = True): defines whether grouped_convolution is to be used or not. combine ('add' or 'concat',default = 'add'): defines the method in which we can combine the channels it has two options add or concat. compresstion_ratio (int,default = 4): The number of channels to be required in the bottleneck channels. Usage: from NiftyTorch.Layers.Convolutional_Layers import Fire import torch in_planes = 20 input = torch.rand(32,20,32,32,32) convolution_block = Fire(inplanes = in_planes, squeeze_planes = 3, expand1x1_planes = 12, expand3x3_planes = 12) output = convolution_block(input) BinActive The BinActive class is for calling binary activation method, which gives the sign of the input tensor as an activation. This is used in Binary Networks and XNOR Network. Usage: import torch from NiftyTorch.Layers.Convolutional_Layers import BinActive input = torch.ones(32,512,32,32,32) activation = BinActive() output = activation(input) BinConv3d Parameters: input_channels (int,required): The number of channels in the input tensor. output_channels (int,required): The number of channels in the output tensor. kernel_size (int,default = 3): Kernel size for the convolution filter. stride (int,default = 1): The number of strides in the convolutional filters. padding (int,default = 0): The padding which is to be done on the ends. groups (int,default = 1): Number of groups in the convolutional filters. dropout (int,default = 0): The dropout probability. Linear (bool,default = False): If True, instead of convolution. Usage: import torch from NiftyTorch.Layers.Convolutional_Layers import BinConv3d in_channels = 512 input = torch.ones(32,in_channels,32,32,32) activation = BinConv3d(input_channels = in_channels,output_channels = 256,kernel_size = 3,stride = 2,padding = 1,groups = 1,dropout = 0.5,Linear = False) output = activation(input)","title":"Layers"},{"location":"layers/#layers","text":"The Layers module contains set of convolutional neural network units, which can used to create custom neural network architectures.","title":"Layers"},{"location":"layers/#modules","text":"","title":"Modules"},{"location":"layers/#conv3x3","text":"This is a simple conv 3x3 layer. Parameters: in_planes (int,required): The number of channels in the input feature map. out_planes (int,requied): The number of channels in the output feature map. stride (int, default = 1): The stride used in each convolution filter. groups (int, default = 1): The number of groups to be considered while performing convolution. dilation (int, default = 1): The field of view to be considered while performing convolution. bias (int, default = False) Usage: from Layers.Convolutional_Layers import Conv3x3 import torch in_planes = 512 input = torch.rand(32,in_planes,32,32,32) convolution = Conv3x3(in_planes = in_planes,out_planes = 32,stride = 1,groups = 1,dilation = 1,bias = False) output = convolution(input)","title":"Conv3x3"},{"location":"layers/#conv1x1","text":"This is a simple conv 1x1 layer. Parameters: in_planes (int,required): The number of channels in the input feature map. out_planes (int,requied): The number of channels in the output feature map. stride (int, default = 1): The stride used in each convolution filter. groups (int, default = 1): The number of groups to be considered while performing convolution. bias (int, default = False) Usage: from NiftyTorch.Layers.Convolutional_Layers import Conv1x1 import torch in_planes = 512 input = torch.rand(32,in_planes,32,32,32) convolution = Conv1x1(in_planes = in_planes,out_planes = 32,stride = 1,groups = 1,bias = False) output = convolution(input)","title":"Conv1x1"},{"location":"layers/#basic-block","text":"BasicBlock is a series of convolution, batchnorm, activation layers with one jump skip connection. The structure of this block is as follows: input -> conv1 -> bn1 -> relu -> conv2 -> bn2 + input -> relu : output Parameters: inplanes (int,required): The number of channels in the input feature map. planes (int,required): The number of channels in the output feature map. stride (int,default = 1): The number of strides to be used in the conv1 downsample (int,default = None): The downsampler to be used to reduce the feature map size F.interpolate groups (int,default = 1): The number of groups to be considered in convolution dilation (int, default = 1): The field of view to be considered while performing convolution. norm_layer (int,default = None): The normalization layer to be used ex: nn.BatchNorm() Usage: from NiftyTorch.Layers.Convolutional_Layers import BasicBlock import torch in_planes = 512 input = torch.rand(32,in_planes,32,32,32) convolution_block = BasicBlock(in_planes = in_planes,planes = 256,out_planes = 32,stride = 1,groups = 1,bias = False) output = convolution_block(input)","title":"Basic Block"},{"location":"layers/#bottleneck-block","text":"BottleNeck is a series of convolution,batchnorm,activation layers with two layer jump skip connection. The structure of this block is as follows: input -> conv1 -> bn1 -> relu -> conv2 -> bn2 -> relu -> conv3 -> bn3 + input -> relu : output Parameters: inplanes (int,required): The number of channels in the input feature map. planes (int,required): The number of channels in the output feature map. stride (int,default = 1): The number of strides to be used in the conv1 downsample (int,default = None): The downsampler to be used to reduce the feature map size F.interpolate groups (int,default = 1): The number of groups to be considered in convolution. base_width (int,default = 1): The base width and expansion are used to calculate the number of filters for conv2 in bottleneck block. dilation (int, default = 1): The field of view to be considered while performing convolution. norm_layer (int,default = None): The normalization layer to be used ex: nn.BatchNorm() expansion (int,default = 1):The base width and expansion are used to calculate the number of filters for conv2 in bottleneck block. Usage: from NiftyTorch.Layers.Convolutional_Layers import BottleNeck import torch in_planes = 512 planes = 256 input = torch.rand(32,in_planes,32,32,32) convolution_block = BottleNeck(inplanes = in_planes, planes = planes, stride=1, downsample=None, groups=1,base_width=64, dilation=1, norm_layer=None,expansion = 1) output = convolution_block(input)","title":"BottleNeck Block"},{"location":"layers/#shuffleunit","text":"ShuffleUnit is the class definition for ShuffleUnit Module, which uses channel shuffling and combination to improve the network performance with fewer parameters. The ShuffleUnit is commonly used in ShuffleNet 1.0 and ShuffleNet 2.0, but they can be used along with any network. For more information, please read the ShuffleNet papers [ ShuffleNet 1.0 , ShuffleNet 2.0 ]. Parameters: in_channels (int,required): number of channels in the input tensor the shuffleunit. out_channels (int,required): number of channels in the tensor generated from the shuffleunit. groups (int,default = 3): define number of groups in which the filters are combined before shuffle operation. grouped_conv (bool,default = True): defines whether grouped_convolution is to be used or not. combine ('add' or 'concat',default = 'add'): defines the method in which we can combine the channels it has two options add or concat. compresstion_ratio (int,default = 4): The number of channels to be required in the bottleneck channels. Usage: from NiftyTorch.Layers.Convolutional_Layers import ShuffleUnit import torch in_planes = 512 planes = 256 input = torch.rand(32,in_planes,32,32,32) convolution_block = ShuffleUnit(inplanes = in_planes, planes = planes, groups=5, grouped_conv=True, combine = 'concat',compression_ratio = 4) output = convolution_block(input)","title":"ShuffleUnit"},{"location":"layers/#fire-module","text":"Fire is the class definition for Fire Module, which uses a combination of 1x1 and 3x3 filters to improve the network performance with fewer parameters. The Fire Module is commonly used in Squeezenet 1.0 and SqueezeNet 2.0 but they can used along with any network. For more information please read the SqueezeNet paper . Parameters: in_channels (int,required): number of channels in the input tensor the shuffleunit. out_channels (int,required): number of channels in the tensor generated from the shuffleunit. groups (int,default = 3): define number of groups in which the filters are combined before shuffle operation. grouped_conv (bool,default = True): defines whether grouped_convolution is to be used or not. combine ('add' or 'concat',default = 'add'): defines the method in which we can combine the channels it has two options add or concat. compresstion_ratio (int,default = 4): The number of channels to be required in the bottleneck channels. Usage: from NiftyTorch.Layers.Convolutional_Layers import Fire import torch in_planes = 20 input = torch.rand(32,20,32,32,32) convolution_block = Fire(inplanes = in_planes, squeeze_planes = 3, expand1x1_planes = 12, expand3x3_planes = 12) output = convolution_block(input)","title":"Fire Module"},{"location":"layers/#binactive","text":"The BinActive class is for calling binary activation method, which gives the sign of the input tensor as an activation. This is used in Binary Networks and XNOR Network. Usage: import torch from NiftyTorch.Layers.Convolutional_Layers import BinActive input = torch.ones(32,512,32,32,32) activation = BinActive() output = activation(input)","title":"BinActive"},{"location":"layers/#binconv3d","text":"Parameters: input_channels (int,required): The number of channels in the input tensor. output_channels (int,required): The number of channels in the output tensor. kernel_size (int,default = 3): Kernel size for the convolution filter. stride (int,default = 1): The number of strides in the convolutional filters. padding (int,default = 0): The padding which is to be done on the ends. groups (int,default = 1): Number of groups in the convolutional filters. dropout (int,default = 0): The dropout probability. Linear (bool,default = False): If True, instead of convolution. Usage: import torch from NiftyTorch.Layers.Convolutional_Layers import BinConv3d in_channels = 512 input = torch.ones(32,in_channels,32,32,32) activation = BinConv3d(input_channels = in_channels,output_channels = 256,kernel_size = 3,stride = 2,padding = 1,groups = 1,dropout = 0.5,Linear = False) output = activation(input)","title":"BinConv3d"},{"location":"license/","text":"License This software is Copyright \u00a9 2020 The University of Southern California. All Rights Reserved. Permission to use, copy, modify, and distribute this software and its documentation for educational, research and non-profit purposes, without fee, and without a written agreement is hereby granted, provided that the above copyright notice, this paragraph and the following three paragraphs appear in all copies. Permission to make commercial use of this software may be obtained by contacting: Farshid Sepehrband farshid.sepehrband@loni.usc.edu University of Southern California 2025 Zonal Ave, Los Angeles, CA 90033, USA This software program and documentation are copyrighted by The University of Southern California. The software program and documentation are supplied \"as is\", without any accompanying services from USC. USC does not warrant that the operation of the program will be uninterrupted or error-free. The end-user understands that the program was developed for research purposes and is advised not to rely exclusively on the program for any reason. IN NO EVENT SHALL THE UNIVERSITY OF SOUTHERN CALIFORNIA BE LIABLE TO ANY PARTY FOR DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES, INCLUDING LOST PROFITS, ARISING OUT OF THE USE OF THIS SOFTWARE AND ITS DOCUMENTATION, EVEN IF THE UNIVERSITY OF SOUTHERN CALIFORNIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. THE UNIVERSITY OF SOUTHERN CALIFORNIA SPECIFICALLY DISCLAIMS ANY WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE SOFTWARE PROVIDED HEREUNDER IS ON AN \"AS IS\" BASIS, AND THE UNIVERSITY OF SOUTHERN CALIFORNIA HAS NO OBLIGATIONS TO PROVIDE MAINTENANCE, SUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS.","title":"Licence"},{"location":"license/#license","text":"This software is Copyright \u00a9 2020 The University of Southern California. All Rights Reserved. Permission to use, copy, modify, and distribute this software and its documentation for educational, research and non-profit purposes, without fee, and without a written agreement is hereby granted, provided that the above copyright notice, this paragraph and the following three paragraphs appear in all copies. Permission to make commercial use of this software may be obtained by contacting: Farshid Sepehrband farshid.sepehrband@loni.usc.edu University of Southern California 2025 Zonal Ave, Los Angeles, CA 90033, USA This software program and documentation are copyrighted by The University of Southern California. The software program and documentation are supplied \"as is\", without any accompanying services from USC. USC does not warrant that the operation of the program will be uninterrupted or error-free. The end-user understands that the program was developed for research purposes and is advised not to rely exclusively on the program for any reason. IN NO EVENT SHALL THE UNIVERSITY OF SOUTHERN CALIFORNIA BE LIABLE TO ANY PARTY FOR DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES, INCLUDING LOST PROFITS, ARISING OUT OF THE USE OF THIS SOFTWARE AND ITS DOCUMENTATION, EVEN IF THE UNIVERSITY OF SOUTHERN CALIFORNIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. THE UNIVERSITY OF SOUTHERN CALIFORNIA SPECIFICALLY DISCLAIMS ANY WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE SOFTWARE PROVIDED HEREUNDER IS ON AN \"AS IS\" BASIS, AND THE UNIVERSITY OF SOUTHERN CALIFORNIA HAS NO OBLIGATIONS TO PROVIDE MAINTENANCE, SUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS.","title":"License"},{"location":"loader/","text":"Loader The Loader class is used to Load data for various types of tasks such as classification, image generation and image segmentation. The Loader class has Classification DataLoader for classification tasks and Segmentation DataLoader for image-to-image (image generation) and image-to-labels (segmentation) tasks. Classification DataLoader For Classification DataLoader the class label can be provided to the model in the form of a csv file, which contains a filename label and classname label . The input file is written as input_image.nii.gz The output file is written as output_labels.csv Parameters: root (str,required): This is path to the directory where the data is stored csv_dir (str,required): This is the csv file which contains the subject and its corresponding label transform (torchvision.transforms,required) : The transforms here is the transforms from torchvision library target_transform (torchvision.transforms,required): The target tranforms is the same as transforms parameter but the transformation is done on the test data loader (function,required): The type of loader to be used, current loader uses nipy. is_valid_file (function,required): List of type of files to be considered to be loaded as an input data. filename_label (str,required): The filename label in the CSV file. class_label (str,required): The class label in the CSV file. file_type (tuple,default = ('nii.gz','.nii')): The file types to be considered as an input for the model. The resulting files are stacked along channels in the input tensor. common (int,default = 64): The common is the size of the 3D Tensor. demographic (list,default = []): The list demographic factors to be used for classification. Usage: from NiftyTorch.Loader.Classification_Loader import ImageFolder from torchvision import transforms data_transforms = transforms.Compose([transforms.ToTensor()]) data_folder = \"../data/train/\" data_csv = \"../data.csv\" filename_label = \"Subject\" class_label = \"labels\" image_scale = 32 file_type = ('t1w.nii.gz','flair.nii.gz') demographic = ['factor1','factor2'] image_datasets = ImageFolder(root = data,data_csv = data_csv,transforms = data_transforms,target_transforms = data_transforms,loader = ,filename_label = filename_label,class_label = class_label,common = image_scale,file_type = file_type,demographic = demographic) Segmentation DataLoader The Segmentation DataLoader is used for image segmentation and image generation. The input file is written as input_image.nii.gz The output file is written as seg.nii.gz Parameters: root (str,required): This is path to the directory where the data is stored. transform (torchvision.transforms,required) : The transforms here is the transforms from torchvision library target_transform (torchvision.transforms,required): The target tranforms is the same as transforms parameter but the transformation is done on the test data loader (function,required): The type of loader to be used, current loader uses nipy. is_valid_file (function,required): List of type of files to be considered to be loaded as an input data. common (int,default = 64): The common is the size of the 3D Tensor. Usage: from NiftyTorch.Loader.Segmentation_DataLoader import ImageFolder from torchvision import transforms data_transforms = transforms.Compose([transforms.ToTensor()]) data_folder = \"../data/train/\" image_scale = 32 image_datasets = ImageFolder(root = data,data_csv = data_csv,transforms = data_transforms,target_transforms = data_transforms,loader = ,filename_label = filename_label,class_label = class_label,common = image_scale)","title":"Loader"},{"location":"loader/#loader","text":"The Loader class is used to Load data for various types of tasks such as classification, image generation and image segmentation. The Loader class has Classification DataLoader for classification tasks and Segmentation DataLoader for image-to-image (image generation) and image-to-labels (segmentation) tasks.","title":"Loader"},{"location":"loader/#classification-dataloader","text":"For Classification DataLoader the class label can be provided to the model in the form of a csv file, which contains a filename label and classname label . The input file is written as input_image.nii.gz The output file is written as output_labels.csv Parameters: root (str,required): This is path to the directory where the data is stored csv_dir (str,required): This is the csv file which contains the subject and its corresponding label transform (torchvision.transforms,required) : The transforms here is the transforms from torchvision library target_transform (torchvision.transforms,required): The target tranforms is the same as transforms parameter but the transformation is done on the test data loader (function,required): The type of loader to be used, current loader uses nipy. is_valid_file (function,required): List of type of files to be considered to be loaded as an input data. filename_label (str,required): The filename label in the CSV file. class_label (str,required): The class label in the CSV file. file_type (tuple,default = ('nii.gz','.nii')): The file types to be considered as an input for the model. The resulting files are stacked along channels in the input tensor. common (int,default = 64): The common is the size of the 3D Tensor. demographic (list,default = []): The list demographic factors to be used for classification. Usage: from NiftyTorch.Loader.Classification_Loader import ImageFolder from torchvision import transforms data_transforms = transforms.Compose([transforms.ToTensor()]) data_folder = \"../data/train/\" data_csv = \"../data.csv\" filename_label = \"Subject\" class_label = \"labels\" image_scale = 32 file_type = ('t1w.nii.gz','flair.nii.gz') demographic = ['factor1','factor2'] image_datasets = ImageFolder(root = data,data_csv = data_csv,transforms = data_transforms,target_transforms = data_transforms,loader = ,filename_label = filename_label,class_label = class_label,common = image_scale,file_type = file_type,demographic = demographic)","title":"Classification DataLoader"},{"location":"loader/#segmentation-dataloader","text":"The Segmentation DataLoader is used for image segmentation and image generation. The input file is written as input_image.nii.gz The output file is written as seg.nii.gz Parameters: root (str,required): This is path to the directory where the data is stored. transform (torchvision.transforms,required) : The transforms here is the transforms from torchvision library target_transform (torchvision.transforms,required): The target tranforms is the same as transforms parameter but the transformation is done on the test data loader (function,required): The type of loader to be used, current loader uses nipy. is_valid_file (function,required): List of type of files to be considered to be loaded as an input data. common (int,default = 64): The common is the size of the 3D Tensor. Usage: from NiftyTorch.Loader.Segmentation_DataLoader import ImageFolder from torchvision import transforms data_transforms = transforms.Compose([transforms.ToTensor()]) data_folder = \"../data/train/\" image_scale = 32 image_datasets = ImageFolder(root = data,data_csv = data_csv,transforms = data_transforms,target_transforms = data_transforms,loader = ,filename_label = filename_label,class_label = class_label,common = image_scale)","title":"Segmentation DataLoader"},{"location":"loss/","text":"Loss The Loss module consists a collection of loss functions which can be used for learning model and optimizing the weights. Modules Weighted Cross Entropy The weighted_cross_entropy loss function weights probabilities for each class with weighted assigned by the user and then normalized by total weight. Parameters: num_class (int,required): The number of classes in the problem. weight (torch.FloatTensor,default = None) : The alpha is the weight to be used as a multiplier for probabilities of each class. Usage: from NiftyTorch.Loss.Losses import weighted_cross_entropy import torch import numpy as np num_classes = 32 weight = np.random.rand(num_classes) input = torch.random.rand(64,num_classes) output = torch.zeroes(64,num_classes) loss = weighted_cross_entropy(num_class = num_classes,weight = weight) print(loss(input,output)) Focal Loss The idea behind FocalLoss is to not only weight the probabilities according to class imbalance, but also take into consideration the difficulty of classifying the example. Parameters: num_class (int,required): The number of classes in the problem. alpha (float,default = None) : The alpha is the weight to be used as a multiplier for probabilities of each class. gamma (float,default = 2.0): The gamma is the exponent to be used in the probabilities. balance_index (int,default = -1): This is the index which needs to be balanced or has the imbalanced (This is used if alpha is a float instead of ndarray). smooth (float,required): The smoothening factor is used to smoothen ground truth labels. size_average (boolean,default = True): The size_average tells whether to average the loss if true else it the sum is returned. Usage: from NiftyTorch.Loss.Losses import FocalLoss import torch import numpy as np num_classes = 32 alpha = np.random.rand(num_classes) input = torch.random.rand(64,num_classes) output = torch.zeroes(64,num_classes) loss = FocalLoss(num_class = num_classes,alpha = alpha,gamma = 2.0,balance_index = 1,smooth = 0.1,size_average = True) print(loss(input,output)) Focal Dice Loss The idea behind FocalDiceLoss is to not only weight the probabilities according to class imbalance, but also take into consideration the difficulty of classifying the example by Parameters: alpha (float,required): The alpha is the weight to be used as a multiplier for dice loss coefficient of each class. beta (float,required): The beta is the exponent to be used in the probabilities. eps (float,required): It is a really small number which decides which used along with denominator in dice loss to avoid division by zero error. num_class (int,required): The number of classes in the problem. Usage: from NiftyTorch.Loss.Losses import FocalDiceLoss import torch import numpy as np num_class = 32 alpha = torch.random.rand(num_class) input = torch.random.rand(64,num_class) output = torch.zeroes(64,num_class) loss = FocalDiceLoss(alpha = alpha,gamma = 2.0,eps = 1e-8,num_class = num_class) print(loss(input,output)) Tversky Loss The tversky_loss function is used to weight false positive and false negative in the loss. Parameters: alpha (float,required): The alpha is the weight to be used as a multiplier for dice loss coefficient of each class. beta (float,required): The beta is the exponent to be used in the probabilities. eps (float,required): It is a really small number which decides which used along with denominator in dice loss to avoid division by zero error. Usage: from NiftyTorch.Loss.Losses import tversky_loss import torch import numpy as np num_class = 32 alpha = torch.random.rand(num_class) input = torch.random.rand(64,num_class) output = torch.zeroes(64,num_class) loss = tversky_loss(alpha = alpha,gamma = 2.0,eps = 1e-8) print(loss(input,output)) Contrastive Loss The ContrastiveLoss function is used in few shot learning paradigm. The inputs of the contrastive loss are two input tensors and target tensor. The target is 0 if they're of different class else it is 1. Parameters: margin (float,required): The margin is the maximum allowed to distance between the input distances. eps (float,default = 1e-9): It is a really small number which decides which used along with denominator in dice loss to avoid division by zero error. size_average (Boolean,default = True): If true, then the loss is averaged or else the sum of the loss is returned. Usage: from NiftyTorch.Loss.Losses import ContrastiveLoss import torch margin = 10.0 input1 = torch.random.rand(64,1024) input2 = torch.random.rand(64,1024) output = torch.zeroes(64) #assuming they're from different class if you they're from same class use torch.ones(64,num_class) loss = ContrastiveLoss(margin = margin,eps = 1e-8,size_average = True) print(loss(input1,input2,output)) Triplet Loss The TripletLoss is used in few shot learning paradigm. The inputs of the triplet loss are two tensor of different classes and an anchor tensor. The distance between the anchor and positive, the distance between the anchor and negative is used assign the class for the anchor. Parameters: margin (float,required): The margin is the maximum allowed to distance between the input distances. size_average (Boolean,default = True): If true, then the loss is averaged or else the sum of the loss is returned. Usage: from NiftyTorch.Loss.Losses import TripletLoss import torch anchor = torch.random.rand(64,1024) #embedding for anchor positive = torch.random.rand(64,1024) #embedding for positive sample negative = torch.random.rand(64,1024) #embedding for negative sample loss = TripletLoss(margin = margin,size_average = True) print(loss(anchor,positive,negative))","title":"Loss"},{"location":"loss/#loss","text":"The Loss module consists a collection of loss functions which can be used for learning model and optimizing the weights.","title":"Loss"},{"location":"loss/#modules","text":"","title":"Modules"},{"location":"loss/#weighted-cross-entropy","text":"The weighted_cross_entropy loss function weights probabilities for each class with weighted assigned by the user and then normalized by total weight. Parameters: num_class (int,required): The number of classes in the problem. weight (torch.FloatTensor,default = None) : The alpha is the weight to be used as a multiplier for probabilities of each class. Usage: from NiftyTorch.Loss.Losses import weighted_cross_entropy import torch import numpy as np num_classes = 32 weight = np.random.rand(num_classes) input = torch.random.rand(64,num_classes) output = torch.zeroes(64,num_classes) loss = weighted_cross_entropy(num_class = num_classes,weight = weight) print(loss(input,output))","title":"Weighted Cross Entropy"},{"location":"loss/#focal-loss","text":"The idea behind FocalLoss is to not only weight the probabilities according to class imbalance, but also take into consideration the difficulty of classifying the example. Parameters: num_class (int,required): The number of classes in the problem. alpha (float,default = None) : The alpha is the weight to be used as a multiplier for probabilities of each class. gamma (float,default = 2.0): The gamma is the exponent to be used in the probabilities. balance_index (int,default = -1): This is the index which needs to be balanced or has the imbalanced (This is used if alpha is a float instead of ndarray). smooth (float,required): The smoothening factor is used to smoothen ground truth labels. size_average (boolean,default = True): The size_average tells whether to average the loss if true else it the sum is returned. Usage: from NiftyTorch.Loss.Losses import FocalLoss import torch import numpy as np num_classes = 32 alpha = np.random.rand(num_classes) input = torch.random.rand(64,num_classes) output = torch.zeroes(64,num_classes) loss = FocalLoss(num_class = num_classes,alpha = alpha,gamma = 2.0,balance_index = 1,smooth = 0.1,size_average = True) print(loss(input,output))","title":"Focal Loss"},{"location":"loss/#focal-dice-loss","text":"The idea behind FocalDiceLoss is to not only weight the probabilities according to class imbalance, but also take into consideration the difficulty of classifying the example by Parameters: alpha (float,required): The alpha is the weight to be used as a multiplier for dice loss coefficient of each class. beta (float,required): The beta is the exponent to be used in the probabilities. eps (float,required): It is a really small number which decides which used along with denominator in dice loss to avoid division by zero error. num_class (int,required): The number of classes in the problem. Usage: from NiftyTorch.Loss.Losses import FocalDiceLoss import torch import numpy as np num_class = 32 alpha = torch.random.rand(num_class) input = torch.random.rand(64,num_class) output = torch.zeroes(64,num_class) loss = FocalDiceLoss(alpha = alpha,gamma = 2.0,eps = 1e-8,num_class = num_class) print(loss(input,output))","title":"Focal Dice Loss"},{"location":"loss/#tversky-loss","text":"The tversky_loss function is used to weight false positive and false negative in the loss. Parameters: alpha (float,required): The alpha is the weight to be used as a multiplier for dice loss coefficient of each class. beta (float,required): The beta is the exponent to be used in the probabilities. eps (float,required): It is a really small number which decides which used along with denominator in dice loss to avoid division by zero error. Usage: from NiftyTorch.Loss.Losses import tversky_loss import torch import numpy as np num_class = 32 alpha = torch.random.rand(num_class) input = torch.random.rand(64,num_class) output = torch.zeroes(64,num_class) loss = tversky_loss(alpha = alpha,gamma = 2.0,eps = 1e-8) print(loss(input,output))","title":"Tversky Loss"},{"location":"loss/#contrastive-loss","text":"The ContrastiveLoss function is used in few shot learning paradigm. The inputs of the contrastive loss are two input tensors and target tensor. The target is 0 if they're of different class else it is 1. Parameters: margin (float,required): The margin is the maximum allowed to distance between the input distances. eps (float,default = 1e-9): It is a really small number which decides which used along with denominator in dice loss to avoid division by zero error. size_average (Boolean,default = True): If true, then the loss is averaged or else the sum of the loss is returned. Usage: from NiftyTorch.Loss.Losses import ContrastiveLoss import torch margin = 10.0 input1 = torch.random.rand(64,1024) input2 = torch.random.rand(64,1024) output = torch.zeroes(64) #assuming they're from different class if you they're from same class use torch.ones(64,num_class) loss = ContrastiveLoss(margin = margin,eps = 1e-8,size_average = True) print(loss(input1,input2,output))","title":"Contrastive Loss"},{"location":"loss/#triplet-loss","text":"The TripletLoss is used in few shot learning paradigm. The inputs of the triplet loss are two tensor of different classes and an anchor tensor. The distance between the anchor and positive, the distance between the anchor and negative is used assign the class for the anchor. Parameters: margin (float,required): The margin is the maximum allowed to distance between the input distances. size_average (Boolean,default = True): If true, then the loss is averaged or else the sum of the loss is returned. Usage: from NiftyTorch.Loss.Losses import TripletLoss import torch anchor = torch.random.rand(64,1024) #embedding for anchor positive = torch.random.rand(64,1024) #embedding for positive sample negative = torch.random.rand(64,1024) #embedding for negative sample loss = TripletLoss(margin = margin,size_average = True) print(loss(anchor,positive,negative))","title":"Triplet Loss"},{"location":"models/","text":"Models The Models module contains a list of model that can be used for Classification, Segmentation and Generation. Modules AlexNet For network information, see AlexNet paper . Parameters: intial_feature_map_size (int,required): The input tensor size. num_classes (int,required): The number of class in the data. in_channels (int,required): The number of channels in the input tensor. strides (list,default = [1,2,1,1,1]): The strides in each convolutional layer of the AlexNet. channels (list,default = [1,2,2,2,1]): The channels in each convolutional layer of the AlexNet. kernel_size (list,default = [3,5,3,3,1]): The size of kernels in each convolutional layer of the AlexNet. padding (list,default = [0,1,1,1,1]): The padding in each convolutional layer of the AlexNet. demographic (list,default = []): The list demographic factors to be used for classification. Usage: from NiftyTorch.Models.AlexNet import AlexNet import torch initial_feature_map = 128 num_classes = 2 in_channels = 1 demographic = ['factor1','factor2'] strides = [1,2,1,1,1] channels = [1,2,2,2,1] kernel_size = [3,5,3,3,1] padding = [0,1,1,1,1] input = torch.random.rand(64,in_channels,initial_feature_map,initial_feature_map,initial_feature_map) model = AlexNet(initial_feature_map,num_classes = num_classes,in_channels = in_channels,strides = stirdes,channels,kernel_size = kernel_size,padding = padding,demographic = demographic) VGGNet For network information, see VGGNet paper . Parameters for VGG_Net: image_scale, cfgs, version, features, num_classes,init_weights image_scale (int,requried): The input tensor size along width size. cfgs (int,required): The configuration of VGG_Net version (str,required): The version can be 'A','B','D' or 'E'. features (torch.Tensor,required): The features from convolution layers. num_classes (int,default = 2): The classes in the dataset. init_weights (bool,default = False): The if true weights are initialzed using kaiming normal or else it is initialized randomly. demographic (list,default = []): The list demographic factors to be used for classification. Parameters for make_layers: cfg (list,required): The configuration for VGG Net (cfgs[version]). in_channels (int,default = 1): The number of channels in the input tensor. batchnorm (boolean,default = True): This boolean variable is used to inidicated whether the batchnorm layer is required. Usage: from NiftyTorch.Models.VGG_Net import VGG from NiftyTorch.Models.VGG_Net import make_layers import torch image_scale = 128 demographic = ['factor1','factor2'] cfgs = {'A':[32,32,32,'M',64,64,64]} version = 'A' in_channels = 1 features = make_layers(cfgs[version],in_channels = in_channels,batchnorm = False) num_classes = 2 init_weights = True input = torch.random.rand(64,in_channels,image_scale,image_scale,image_scale) model = VGG(image_scale,cfgs = cfgs,version = version,features = featurs,num_classes = num_classes,init_weights = init_weights,demographic = demographic) ResNet For network information, see ResNet paper . Parameters: block (model,required): The block can be BasicBlock or BottleNeck Layers. layers (int,required): The number of layers in the block layers. stride (int,required): The stride at each layer the size of stride is same as the number of layers. in_channels (int,required): The number of channels in the input data. num_classes (int,required): The number of class in the data. zero_init_residual (bool,default = False): This variable decides whether the batchnorm layer is to be initialized. groups (int,default = 1): The number of groups to be considered in the block layer. width_per_group (int,default = 64): The number of channels in each block layer. replace_stride_with_dilation (bool,default = False): The dilation at each block layer. norm_layer (torch.nn,default = None): The type of norm layer to be used. demographic (list,default = []): The list demographic factors to be used for classification. Usage: from NiftyTorch.Models.ResNet import ResNet import torch import torch.nn as nn from NiftyTorch.Layers.Convolutional_Layers import BottleNeck block = BottleNeck demographic = ['factor1','factor2'] layers = [1,2,1,1,2] stride = [1,1,1,1,1] num_classes = 2 zero_init_residual = True groups = 1 replace_stride_with_dilation = [2,2,2,2,2] norm_layer = nn.Batchnorm in_channels = 1 input = torch.random.rand(64,in_channels,32,32,32) model = ResNet(block = block,layers = layers,stride = stride,in_channels = 1,num_classes = num_classes,zero_init_residual = zero_init_residual,groups = groups,replace_stride_with_dilation = replace_stride_with_dilation,norm_layer = norm_layer,demographic = demographic) ShuffleNet For network information, see ShuffleNet paper . Parameters: stage_repeats (list,required): The number of times each stage is repeated groups (int, deafult = 3): number of groups to be used in grouped 1x1 convolutions in each ShuffleUnit. in_channels (int, deafult = 1): number of channels in the input tensor. num_classes (int, default = 2): number of classes to predict. demographic (list,default = []): The list demographic factors to be used for classification. Usage: from NiftyTorch.Models.ShuffleNet import ShuffleNet import torch import torch.nn as nn stage_repeats = [3,7,3] groups = 5 num_classes = 2 demographic = ['factor1','factor2'] in_channels = 1 model = ShuffleNet(stage_repeats = stage_repeats,groups = groups,in_channels = 1,num_classes = num_classes,demographic = demographic) SqueezeNet For network information, see SqueezeNet paper . Parameters: version (str,default = '1_0'): The version of shuffleNet to be used. num_classes (str, deafult = 2): Number of classes in the dataset. in_channels (int, deafult = 1): number of channels in the input tensor. Usage: from NiftyTorch.Models.SqueezeNet import SqueezeNet import torch import torch.nn as nn version = '1_1' num_classes = 2 in_channels = 1 model = SqueezeNet(version = version,num_classes = 2,in_channels = 1) XNOR-Net For network information, see XNOR-Net paper . Parameters: image_scale: The input tensor size along width size. num_classes: Number of classes in the dataset. in_channels: number of channels in the input tensor. channels: The channels in each convolutional layer of the AlexNet. kernel_size: The size of kernels in each convolutional layer of the AlexNet. strides: The strides in each convolutional layer of the AlexNet. padding: The padding in each convolutional layer of the AlexNet. groups: The number of groups to be considered in the block layer. demographic (list,default = []): The list demographic factors to be used for classification. Usage: from NiftyTorch.Models.XNOR_NET import AlexNet image_scale = 128 num_classes = 2 in_channels = 1 demographic = ['factor1','factor2'] channels = [8,16,24,32,32,32] kernel_size = [11, 5, 3, 3, 3] strides = [4, 1, 1, 1, 1] padding = [0, 2, 1, 1, 1] groups = [1, 1, 1, 1, 1] AlexNet(image_scale = image_scale,num_classes = num_classes,in_channels = in_channels,channels = channels,kernel_size = kernel_size,strides = strides,padding = padding,groups = groups,demographic = demographic)","title":"Models"},{"location":"models/#models","text":"The Models module contains a list of model that can be used for Classification, Segmentation and Generation.","title":"Models"},{"location":"models/#modules","text":"","title":"Modules"},{"location":"models/#alexnet","text":"For network information, see AlexNet paper . Parameters: intial_feature_map_size (int,required): The input tensor size. num_classes (int,required): The number of class in the data. in_channels (int,required): The number of channels in the input tensor. strides (list,default = [1,2,1,1,1]): The strides in each convolutional layer of the AlexNet. channels (list,default = [1,2,2,2,1]): The channels in each convolutional layer of the AlexNet. kernel_size (list,default = [3,5,3,3,1]): The size of kernels in each convolutional layer of the AlexNet. padding (list,default = [0,1,1,1,1]): The padding in each convolutional layer of the AlexNet. demographic (list,default = []): The list demographic factors to be used for classification. Usage: from NiftyTorch.Models.AlexNet import AlexNet import torch initial_feature_map = 128 num_classes = 2 in_channels = 1 demographic = ['factor1','factor2'] strides = [1,2,1,1,1] channels = [1,2,2,2,1] kernel_size = [3,5,3,3,1] padding = [0,1,1,1,1] input = torch.random.rand(64,in_channels,initial_feature_map,initial_feature_map,initial_feature_map) model = AlexNet(initial_feature_map,num_classes = num_classes,in_channels = in_channels,strides = stirdes,channels,kernel_size = kernel_size,padding = padding,demographic = demographic)","title":"AlexNet"},{"location":"models/#vggnet","text":"For network information, see VGGNet paper . Parameters for VGG_Net: image_scale, cfgs, version, features, num_classes,init_weights image_scale (int,requried): The input tensor size along width size. cfgs (int,required): The configuration of VGG_Net version (str,required): The version can be 'A','B','D' or 'E'. features (torch.Tensor,required): The features from convolution layers. num_classes (int,default = 2): The classes in the dataset. init_weights (bool,default = False): The if true weights are initialzed using kaiming normal or else it is initialized randomly. demographic (list,default = []): The list demographic factors to be used for classification. Parameters for make_layers: cfg (list,required): The configuration for VGG Net (cfgs[version]). in_channels (int,default = 1): The number of channels in the input tensor. batchnorm (boolean,default = True): This boolean variable is used to inidicated whether the batchnorm layer is required. Usage: from NiftyTorch.Models.VGG_Net import VGG from NiftyTorch.Models.VGG_Net import make_layers import torch image_scale = 128 demographic = ['factor1','factor2'] cfgs = {'A':[32,32,32,'M',64,64,64]} version = 'A' in_channels = 1 features = make_layers(cfgs[version],in_channels = in_channels,batchnorm = False) num_classes = 2 init_weights = True input = torch.random.rand(64,in_channels,image_scale,image_scale,image_scale) model = VGG(image_scale,cfgs = cfgs,version = version,features = featurs,num_classes = num_classes,init_weights = init_weights,demographic = demographic)","title":"VGGNet"},{"location":"models/#resnet","text":"For network information, see ResNet paper . Parameters: block (model,required): The block can be BasicBlock or BottleNeck Layers. layers (int,required): The number of layers in the block layers. stride (int,required): The stride at each layer the size of stride is same as the number of layers. in_channels (int,required): The number of channels in the input data. num_classes (int,required): The number of class in the data. zero_init_residual (bool,default = False): This variable decides whether the batchnorm layer is to be initialized. groups (int,default = 1): The number of groups to be considered in the block layer. width_per_group (int,default = 64): The number of channels in each block layer. replace_stride_with_dilation (bool,default = False): The dilation at each block layer. norm_layer (torch.nn,default = None): The type of norm layer to be used. demographic (list,default = []): The list demographic factors to be used for classification. Usage: from NiftyTorch.Models.ResNet import ResNet import torch import torch.nn as nn from NiftyTorch.Layers.Convolutional_Layers import BottleNeck block = BottleNeck demographic = ['factor1','factor2'] layers = [1,2,1,1,2] stride = [1,1,1,1,1] num_classes = 2 zero_init_residual = True groups = 1 replace_stride_with_dilation = [2,2,2,2,2] norm_layer = nn.Batchnorm in_channels = 1 input = torch.random.rand(64,in_channels,32,32,32) model = ResNet(block = block,layers = layers,stride = stride,in_channels = 1,num_classes = num_classes,zero_init_residual = zero_init_residual,groups = groups,replace_stride_with_dilation = replace_stride_with_dilation,norm_layer = norm_layer,demographic = demographic)","title":"ResNet"},{"location":"models/#shufflenet","text":"For network information, see ShuffleNet paper . Parameters: stage_repeats (list,required): The number of times each stage is repeated groups (int, deafult = 3): number of groups to be used in grouped 1x1 convolutions in each ShuffleUnit. in_channels (int, deafult = 1): number of channels in the input tensor. num_classes (int, default = 2): number of classes to predict. demographic (list,default = []): The list demographic factors to be used for classification. Usage: from NiftyTorch.Models.ShuffleNet import ShuffleNet import torch import torch.nn as nn stage_repeats = [3,7,3] groups = 5 num_classes = 2 demographic = ['factor1','factor2'] in_channels = 1 model = ShuffleNet(stage_repeats = stage_repeats,groups = groups,in_channels = 1,num_classes = num_classes,demographic = demographic)","title":"ShuffleNet"},{"location":"models/#squeezenet","text":"For network information, see SqueezeNet paper . Parameters: version (str,default = '1_0'): The version of shuffleNet to be used. num_classes (str, deafult = 2): Number of classes in the dataset. in_channels (int, deafult = 1): number of channels in the input tensor. Usage: from NiftyTorch.Models.SqueezeNet import SqueezeNet import torch import torch.nn as nn version = '1_1' num_classes = 2 in_channels = 1 model = SqueezeNet(version = version,num_classes = 2,in_channels = 1)","title":"SqueezeNet"},{"location":"models/#xnor-net","text":"For network information, see XNOR-Net paper . Parameters: image_scale: The input tensor size along width size. num_classes: Number of classes in the dataset. in_channels: number of channels in the input tensor. channels: The channels in each convolutional layer of the AlexNet. kernel_size: The size of kernels in each convolutional layer of the AlexNet. strides: The strides in each convolutional layer of the AlexNet. padding: The padding in each convolutional layer of the AlexNet. groups: The number of groups to be considered in the block layer. demographic (list,default = []): The list demographic factors to be used for classification. Usage: from NiftyTorch.Models.XNOR_NET import AlexNet image_scale = 128 num_classes = 2 in_channels = 1 demographic = ['factor1','factor2'] channels = [8,16,24,32,32,32] kernel_size = [11, 5, 3, 3, 3] strides = [4, 1, 1, 1, 1] padding = [0, 2, 1, 1, 1] groups = [1, 1, 1, 1, 1] AlexNet(image_scale = image_scale,num_classes = num_classes,in_channels = in_channels,channels = channels,kernel_size = kernel_size,strides = strides,padding = padding,groups = groups,demographic = demographic)","title":"XNOR-Net"},{"location":"training/","text":"Training Generic Training This section describes the parameters common to all the models present in the Models class in NiftyTorch. Parameters: num_classes (int,required): The number of classes in a datasets. in_channels (int,required): The number of channels in the input to the model. data_folder (str,required): The path to the directory which contains input data folder. data_csv (str,required): The path to the csv containing the filename and it's corresponding label. data_transforms (torchvision.transforms,required): The transformations from torchvision which is to be applied to the dataset. filename_label (str,required): The label which used to identify the input image file name. class_label (str,required): The label which is used to identify the class information for the corresponding filename. learning_rate (float,default = 3e-4): The learning which is used to be for the optimizer. step_size (int,default = 7): The step size to used for step based learning rate scheduler. gamma (float,default = 0.2): The reduction factor to be used in step based learning rate scheduler. cuda (str,default = None): The which GPU is to be used. batch_size (int,default = 1): The number examples to be used in each gradient update. image_scale (int,default = 128): The size of the image to be considerd for rescaling the image. loss (torch.nn,default = nn.CrossEntropyLoss()): The loss function be used in the required task. optimizer (torch.optim,default = optim.Adam): The optimizer which is used for updating weights. device_ids (list,default = []): The list of GPUs to be considered for data parallelization. l2 (float,default = 0): The l2 regularization coefficient. Modules AlexNet Training Parameters Parameters: channels (list,default = [1,2,2,2,1]): A list containing the out_channels for each convolutional layer, it must be of size 5. kernel_size (list,default = [3,5,3,3,1]): A list containing the kernel size of each convolutional layer, it must be of size 5. strides (list,default = [1,2,1,1,1]): A list containing the stride at each convolutional layer, it must be of size 5. padding (list,default = [0,1,1,1,1]): A list containing the padding for each convolutional layer, it must be of size 5. Usage: import torch from Models.AlexNet import train_alexnet from torchvision import transforms data_transforms = transforms.Compose([transforms.ToTensor()]) data_folder = \"./data\" data_csv = \"./data.csv\" train = train_alexnet() train.set_params(num_classes = 2, in_channels = 1, data_folder = data_folder, data_csv = data_csv,channels = [1,2,4,2,1],kernel_size = [3,5,5,3,1],strides = [1,2,2,2,1],padding = [1,1,1,1,1], data_transforms = data_transforms, filename_label = 'Subject',class_label = 'Class',learning_rate = 3e-4,step_size = 7, gamma = 0.1, cuda = 'cuda:3',batch_size = 16,image_scale = 128) train.train() VGGNet Training Parameters Parameters: version (str,default = \"A\"): The version can be 'A','B','D' or 'E'. cfgs (list,default = same network parameters): A list containing the configurations. Usage: import torch from Models.VGG_Net import train_vggnet from torchvision import transforms data_transforms = transforms.Compose([transforms.ToTensor()]) data_folder = \"./data\" data_csv = \"./data.csv\" cfgs = {'B':[4, 'M', 8, 'M', 8, 8, 'M', 32, 32, 'M', 32, 64, 'M']} train = train_vggnet() train.set_params(num_classes = 2, in_channels = 1, data_folder = data_folder, data_csv = data_csv,version = \"B\", data_transforms = data_transforms, filename_label = 'Subject',class_label = 'Class',learning_rate = 3e-4,step_size = 7, gamma = 0.1, cuda = 'cuda:3',batch_size = 16,image_scale = 128,cfgs = cfgs) train.train() ResNet Training Parameters Parameters: block (default = BottleNeck): The type of network module to be used as a building block in the resnet. layers (list,default = [1,2,4,4]): The how many times does each block has to be repeated in the resnet. stride (list,default = [2,1,2,2,2]): The stride to be used in each building block of the resnet. channels (list,default = [64,128,256,512]): The number of channels to be maintained in each building block of the resnet. Usage: import torch from Models.ResNet import train_resnet from torchvision import transforms from NiftyTorch.Layers.Convolutional_Layers import BottleNeck data_transforms = transforms.Compose([transforms.ToTensor()]) layers = [1,2,1,1,2] stride = [1,1,1,1,1] channels = [32,64,64,32,32] data_folder = \"./data\" data_csv = \"./data.csv\" train = train_resnet() train.set_params(num_classes = 2, in_channels = 1, data_folder = data_folder, data_csv = data_csv,block = block, data_transforms = data_transforms, filename_label = 'Subject',class_label = 'Class',layers = [1,2,4,4],stride = [2,1,2,2,2],channels = [64,128,256,512],learning_rate = 3e-4,step_size = 7, gamma = 0.1, cuda = 'cuda:3',batch_size = 16,image_scale = 128) train.train() ShuffleNet Training Parameters Parameters: groups (default = 2): number of groups to be used in grouped 1x1 convolutions in each ShuffleUnit. stage_repeats (list,default = [3,7,3]): The number of times each stage is repeated. Usage: import torch from Models.ShuffleNet import train_shufflenet from torchvision import transforms data_transforms = transforms.Compose([transforms.ToTensor()]) groups = 2 stage_repeats = [2,7,4] data_folder = \"./data\" data_csv = \"./data.csv\" train = train_shufflenet() train.set_params(num_classes = 2, in_channels = 1, data_folder = data_folder, data_csv = data_csv,groups = groups, data_transforms = data_transforms, filename_label = 'Subject',class_label = 'Class',stage_repeats = stage_repeats,learning_rate = 3e-4,step_size = 7, gamma = 0.1, cuda = 'cuda:3',batch_size = 16,image_scale = 128) train.train() SqueezeNet Training Parameters Parameters: version (str,default = '1_0'): The version of squeezenet to be used. Usage: import torch from Models.SqueezeNet import train_squeezenet from torchvision import transforms data_transforms = transforms.Compose([transforms.ToTensor()]) version = '1_1' data_folder = \"./data\" data_csv = \"./data.csv\" train = train_squeezenet() train.set_params(num_classes = 2, in_channels = 1, data_folder = data_folder, data_csv = data_csv,version = version, data_transforms = data_transforms, filename_label = 'Subject',class_label = 'Class',learning_rate = 3e-4,step_size = 7, gamma = 0.1, cuda = 'cuda:3',batch_size = 16,image_scale = 128) train.train() XNOR NET Parameters: channels (list,default = [32, 96, 144, 144, 96, 7]): A list containing the out_channels for each convolutional layer, it must be of size 5. kernel_size (list,default = [11, 5, 3, 3, 3]): A list containing the kernel size of each convolutional layer, it must be of size 5. strides (list,default = [4, 1, 1, 1, 1]): A list containing the stride at each convolutional layer, it must be of size 5. padding (list,default = [0, 2, 1, 1, 1]): A list containing the padding for each convolutional layer, it must be of size 5. groups (list,default = [1, 1, 1, 1, 1]): A list containing the number of groups in convolution filters for each convolutional layer, it must be of size 5. Usage: import torch from Models.XNOR_NET import train_xnornet from torchvision import transforms data_transforms = transforms.Compose([transforms.ToTensor()]) version = '1_1' data_folder = \"./data\" data_csv = \"./data.csv\" train = train_xnornet() train.set_params(num_classes = 2, in_channels = 1, data_folder = data_folder, data_csv = data_csv,channels = [32,96,144,42,6],kernel_size = [3,5,5,3,1],strides = [1,2,1,2,1],padding = [1,1,0,1,1],groups = [1,2,2,1,1] data_transforms = data_transforms, filename_label = 'Subject',class_label = 'Class',learning_rate = 3e-4,step_size = 7, gamma = 0.1, cuda = 'cuda:3',batch_size = 16,image_scale = 128) train.train() Hyperparameter Training Generic Hyperparameter Tuning parameters For hyperparameter tuning we need to create a configuration dictionary where below parameters are the keys. These are generic parameters for hyperparameter tuning: learning_rate (bool/float): If False, the hyperparameter tuning is considered for learning rate else for any other float value or True. lr_min (float): Minimum learning rate to be considered for tuning. lr_max (float): Maximum learning rate to be considered for tuning. batch_size (bool/int): If False, the hyperparameter tuning is considered for batch size else for any other int value or True. data_folder (string): The path to the directory which contains input data folder data_csv (string): The path to the csv containing the filename and it's corresponding label. gamma (float): The reduction factor to be used in step based learning rate scheduler. num_classes (int): The number of classes in a datasets. loss (bool): If False, the hyperparameter tuning is considered for loss else a single loss function is considered. step_size (float): The step size to used for step based learning rate scheduler loss_list (list): The list of losses to be considered for training. scheduler (bool): If False, the hyperparameter tuning is considered for scheduler else single scheduler function is considered. scheduler_list (list): The list of scheduler to be considered for hyperparameter tuning. optimizer (bool/nn.optim): If False, the hyperparameter tuning is considered for optimizer else a single optimizer function is considered. opt_list (list): The list of optimizer to be considered for optimization. filename_label (str): The label which used to identify the input image file name. class_label (str): The label which is used to identify the class information for the corresponding filename. in_channels (int): The number of channels in the input to the model. num_workers (int): The threads to be considered while loading the data. image_scale (bool): The size of the image to be considerd for rescaling the image. image_scale_list (list): The list of the image sizes to be considered for hyperparameter tuning. device_ids (list): The list of devices to be considered for data parallelization. cuda (str): The GPU to be considered for loading data. l2 (float,default = 0): The l2 regularization coefficient. AlexNet Hyparameter Tuning These are hyperparameters for AlexNet: channels (int/bool): If False, the hyperparameter tuning is considered for channels. channels_1 (list): The list containing all values to be tested for channel 1. channels_2 (list): The list containing all values to be tested for channel 2. channels_3 (list): The list containing all values to be tested for channel 3. channels_4 (list): The list containing all values to be tested for channel 4. channels_5 (list): The list containing all values to be tested for channel 5. strides (bool): If False, the hyperparameter tuning is considered for strides. strides_1 (list): The list containing all values to be tested for strides 1. strides_2 (list): The list containing all values to be tested for strides 2. strides_3 (list): The list containing all values to be tested for strides 3. strides_4 (list): The list containing all values to be tested for strides 4. strides_5 (list): The list containing all values to be tested for strides 4. kernel_size (bool): If False, the hyperparameter tuning is considered for kernel size. kernel_size_1 (list): The list containing all values to be tested for kernel size 1. kernel_size_2 (list): The list containing all values to be tested for kernel size 2. kernel_size_3 (list): The list containing all values to be tested for kernel size 3. kernel_size_4 (list): The list containing all values to be tested for kernel size 4. kernel_size_5 (list): The list containing all values to be tested for kernel size 5. padding (bool): If False, the hyperparameter tuning is considered for padding. padding_1 (list): The list containing all values to be tested for padding 1. padding_2 (list): The list containing all values to be tested for padding 2. padding_3 (list): The list containing all values to be tested for padding 3. padding_4 (list): The list containing all values to be tested for padding 4. padding_5 (list): The list containing all values to be tested for padding 5. ResNet Hyperparameter Tuning These are hyperparameters for ResNet: groups (bool): If True, the hyperparameter tuning is considered for groups. groups_min (int): The minimum group value to be used for hyperparameter tuning. groups_max (int): The maximum group value to be used for hyperparameter tuning. block (bool): If True, the type of block in resent is considered for hyperparameter. block_list (list): The list containing different type of blocks to be considered for hyperparameter tuning. norm_layer (bool): If True, the type of normalization layers in resent is considered for hyperparameter. norm_layer_list (list): The list containing different type of normalization layers to be considered for hyperparameter tuning. width_per_group (bool): If True, the number of groups per each layer in resent is considered for hyperparameter. width_per_group_list (list): The list containing different types of groups. ShuffleNet Hyperparameter Tuning These are hyperparameters for ShuffleNet: groups (bool): If True, the number of groups per each layer in resent is considered for hyperparameter. groups_min (int): The minimum group value to be used for hyperparameter tuning. groups_max (int): The maximum group value to be used for hyperparameter tuning. stage_repeat (bool): If True, the number of stage_repeats in shufflenet is considered for hyperparameter. stage_repeat_1 (list): The list containing all the values stage_repeats_1. stage_repeat_2 (list): The list containing all the values stage_repeats_2. stage_repeat_3 (list): The list containing all the values stage_repeats_3. XNOR NET Hyperparameter Tuning channels (int/bool): If False, the hyperparameter tuning is considered for channels. channels_1 (list): The list containing all values to be tested for channel 1. channels_2 (list): The list containing all values to be tested for channel 2. channels_3 (list): The list containing all values to be tested for channel 3. channels_4 (list): The list containing all values to be tested for channel 4. channels_5 (list): The list containing all values to be tested for channel 5. strides (bool): If False, the hyperparameter tuning is considered for strides. strides_1 (list): The list containing all values to be tested for strides 1. strides_2 (list): The list containing all values to be tested for strides 2. strides_3 (list): The list containing all values to be tested for strides 3. strides_4 (list): The list containing all values to be tested for strides 4. strides_5 (list): The list containing all values to be tested for strides 4. kernel_size (bool): If False, the hyperparameter tuning is considered for kernel size. kernel_size_1 (list): The list containing all values to be tested for kernel size 1. kernel_size_2 (list): The list containing all values to be tested for kernel size 2. kernel_size_3 (list): The list containing all values to be tested for kernel size 3. kernel_size_4 (list): The list containing all values to be tested for kernel size 4. kernel_size_5 (list): The list containing all values to be tested for kernel size 5. padding (bool): If False, the hyperparameter tuning is considered for padding. padding_1 (list): The list containing all values to be tested for padding 1. padding_2 (list): The list containing all values to be tested for padding 2. padding_3 (list): The list containing all values to be tested for padding 3. padding_4 (list): The list containing all values to be tested for padding 4. padding_5 (list): The list containing all values to be tested for padding 5. groups (bool): If False, the hyperparameter tuning is considered for groups. groups_1 (list): The list containing all values to be tested for groups 1. groups_2 (list): The list containing all values to be tested for groups 2. groups_3 (list): The list containing all values to be tested for groups 3. groups_4 (list): The list containing all values to be tested for groups 4. groups_5 (list): The list containing all values to be tested for groups 5.","title":"Training"},{"location":"training/#training","text":"","title":"Training"},{"location":"training/#generic-training","text":"This section describes the parameters common to all the models present in the Models class in NiftyTorch. Parameters: num_classes (int,required): The number of classes in a datasets. in_channels (int,required): The number of channels in the input to the model. data_folder (str,required): The path to the directory which contains input data folder. data_csv (str,required): The path to the csv containing the filename and it's corresponding label. data_transforms (torchvision.transforms,required): The transformations from torchvision which is to be applied to the dataset. filename_label (str,required): The label which used to identify the input image file name. class_label (str,required): The label which is used to identify the class information for the corresponding filename. learning_rate (float,default = 3e-4): The learning which is used to be for the optimizer. step_size (int,default = 7): The step size to used for step based learning rate scheduler. gamma (float,default = 0.2): The reduction factor to be used in step based learning rate scheduler. cuda (str,default = None): The which GPU is to be used. batch_size (int,default = 1): The number examples to be used in each gradient update. image_scale (int,default = 128): The size of the image to be considerd for rescaling the image. loss (torch.nn,default = nn.CrossEntropyLoss()): The loss function be used in the required task. optimizer (torch.optim,default = optim.Adam): The optimizer which is used for updating weights. device_ids (list,default = []): The list of GPUs to be considered for data parallelization. l2 (float,default = 0): The l2 regularization coefficient.","title":"Generic Training"},{"location":"training/#modules","text":"","title":"Modules"},{"location":"training/#alexnet-training-parameters","text":"Parameters: channels (list,default = [1,2,2,2,1]): A list containing the out_channels for each convolutional layer, it must be of size 5. kernel_size (list,default = [3,5,3,3,1]): A list containing the kernel size of each convolutional layer, it must be of size 5. strides (list,default = [1,2,1,1,1]): A list containing the stride at each convolutional layer, it must be of size 5. padding (list,default = [0,1,1,1,1]): A list containing the padding for each convolutional layer, it must be of size 5. Usage: import torch from Models.AlexNet import train_alexnet from torchvision import transforms data_transforms = transforms.Compose([transforms.ToTensor()]) data_folder = \"./data\" data_csv = \"./data.csv\" train = train_alexnet() train.set_params(num_classes = 2, in_channels = 1, data_folder = data_folder, data_csv = data_csv,channels = [1,2,4,2,1],kernel_size = [3,5,5,3,1],strides = [1,2,2,2,1],padding = [1,1,1,1,1], data_transforms = data_transforms, filename_label = 'Subject',class_label = 'Class',learning_rate = 3e-4,step_size = 7, gamma = 0.1, cuda = 'cuda:3',batch_size = 16,image_scale = 128) train.train()","title":"AlexNet Training Parameters"},{"location":"training/#vggnet-training-parameters","text":"Parameters: version (str,default = \"A\"): The version can be 'A','B','D' or 'E'. cfgs (list,default = same network parameters): A list containing the configurations. Usage: import torch from Models.VGG_Net import train_vggnet from torchvision import transforms data_transforms = transforms.Compose([transforms.ToTensor()]) data_folder = \"./data\" data_csv = \"./data.csv\" cfgs = {'B':[4, 'M', 8, 'M', 8, 8, 'M', 32, 32, 'M', 32, 64, 'M']} train = train_vggnet() train.set_params(num_classes = 2, in_channels = 1, data_folder = data_folder, data_csv = data_csv,version = \"B\", data_transforms = data_transforms, filename_label = 'Subject',class_label = 'Class',learning_rate = 3e-4,step_size = 7, gamma = 0.1, cuda = 'cuda:3',batch_size = 16,image_scale = 128,cfgs = cfgs) train.train()","title":"VGGNet Training Parameters"},{"location":"training/#resnet-training-parameters","text":"Parameters: block (default = BottleNeck): The type of network module to be used as a building block in the resnet. layers (list,default = [1,2,4,4]): The how many times does each block has to be repeated in the resnet. stride (list,default = [2,1,2,2,2]): The stride to be used in each building block of the resnet. channels (list,default = [64,128,256,512]): The number of channels to be maintained in each building block of the resnet. Usage: import torch from Models.ResNet import train_resnet from torchvision import transforms from NiftyTorch.Layers.Convolutional_Layers import BottleNeck data_transforms = transforms.Compose([transforms.ToTensor()]) layers = [1,2,1,1,2] stride = [1,1,1,1,1] channels = [32,64,64,32,32] data_folder = \"./data\" data_csv = \"./data.csv\" train = train_resnet() train.set_params(num_classes = 2, in_channels = 1, data_folder = data_folder, data_csv = data_csv,block = block, data_transforms = data_transforms, filename_label = 'Subject',class_label = 'Class',layers = [1,2,4,4],stride = [2,1,2,2,2],channels = [64,128,256,512],learning_rate = 3e-4,step_size = 7, gamma = 0.1, cuda = 'cuda:3',batch_size = 16,image_scale = 128) train.train()","title":"ResNet Training Parameters"},{"location":"training/#shufflenet-training-parameters","text":"Parameters: groups (default = 2): number of groups to be used in grouped 1x1 convolutions in each ShuffleUnit. stage_repeats (list,default = [3,7,3]): The number of times each stage is repeated. Usage: import torch from Models.ShuffleNet import train_shufflenet from torchvision import transforms data_transforms = transforms.Compose([transforms.ToTensor()]) groups = 2 stage_repeats = [2,7,4] data_folder = \"./data\" data_csv = \"./data.csv\" train = train_shufflenet() train.set_params(num_classes = 2, in_channels = 1, data_folder = data_folder, data_csv = data_csv,groups = groups, data_transforms = data_transforms, filename_label = 'Subject',class_label = 'Class',stage_repeats = stage_repeats,learning_rate = 3e-4,step_size = 7, gamma = 0.1, cuda = 'cuda:3',batch_size = 16,image_scale = 128) train.train()","title":"ShuffleNet Training Parameters"},{"location":"training/#squeezenet-training-parameters","text":"Parameters: version (str,default = '1_0'): The version of squeezenet to be used. Usage: import torch from Models.SqueezeNet import train_squeezenet from torchvision import transforms data_transforms = transforms.Compose([transforms.ToTensor()]) version = '1_1' data_folder = \"./data\" data_csv = \"./data.csv\" train = train_squeezenet() train.set_params(num_classes = 2, in_channels = 1, data_folder = data_folder, data_csv = data_csv,version = version, data_transforms = data_transforms, filename_label = 'Subject',class_label = 'Class',learning_rate = 3e-4,step_size = 7, gamma = 0.1, cuda = 'cuda:3',batch_size = 16,image_scale = 128) train.train()","title":"SqueezeNet Training Parameters"},{"location":"training/#xnor-net","text":"Parameters: channels (list,default = [32, 96, 144, 144, 96, 7]): A list containing the out_channels for each convolutional layer, it must be of size 5. kernel_size (list,default = [11, 5, 3, 3, 3]): A list containing the kernel size of each convolutional layer, it must be of size 5. strides (list,default = [4, 1, 1, 1, 1]): A list containing the stride at each convolutional layer, it must be of size 5. padding (list,default = [0, 2, 1, 1, 1]): A list containing the padding for each convolutional layer, it must be of size 5. groups (list,default = [1, 1, 1, 1, 1]): A list containing the number of groups in convolution filters for each convolutional layer, it must be of size 5. Usage: import torch from Models.XNOR_NET import train_xnornet from torchvision import transforms data_transforms = transforms.Compose([transforms.ToTensor()]) version = '1_1' data_folder = \"./data\" data_csv = \"./data.csv\" train = train_xnornet() train.set_params(num_classes = 2, in_channels = 1, data_folder = data_folder, data_csv = data_csv,channels = [32,96,144,42,6],kernel_size = [3,5,5,3,1],strides = [1,2,1,2,1],padding = [1,1,0,1,1],groups = [1,2,2,1,1] data_transforms = data_transforms, filename_label = 'Subject',class_label = 'Class',learning_rate = 3e-4,step_size = 7, gamma = 0.1, cuda = 'cuda:3',batch_size = 16,image_scale = 128) train.train()","title":"XNOR NET"},{"location":"training/#hyperparameter-training","text":"","title":"Hyperparameter Training"},{"location":"training/#generic-hyperparameter-tuning-parameters","text":"For hyperparameter tuning we need to create a configuration dictionary where below parameters are the keys. These are generic parameters for hyperparameter tuning: learning_rate (bool/float): If False, the hyperparameter tuning is considered for learning rate else for any other float value or True. lr_min (float): Minimum learning rate to be considered for tuning. lr_max (float): Maximum learning rate to be considered for tuning. batch_size (bool/int): If False, the hyperparameter tuning is considered for batch size else for any other int value or True. data_folder (string): The path to the directory which contains input data folder data_csv (string): The path to the csv containing the filename and it's corresponding label. gamma (float): The reduction factor to be used in step based learning rate scheduler. num_classes (int): The number of classes in a datasets. loss (bool): If False, the hyperparameter tuning is considered for loss else a single loss function is considered. step_size (float): The step size to used for step based learning rate scheduler loss_list (list): The list of losses to be considered for training. scheduler (bool): If False, the hyperparameter tuning is considered for scheduler else single scheduler function is considered. scheduler_list (list): The list of scheduler to be considered for hyperparameter tuning. optimizer (bool/nn.optim): If False, the hyperparameter tuning is considered for optimizer else a single optimizer function is considered. opt_list (list): The list of optimizer to be considered for optimization. filename_label (str): The label which used to identify the input image file name. class_label (str): The label which is used to identify the class information for the corresponding filename. in_channels (int): The number of channels in the input to the model. num_workers (int): The threads to be considered while loading the data. image_scale (bool): The size of the image to be considerd for rescaling the image. image_scale_list (list): The list of the image sizes to be considered for hyperparameter tuning. device_ids (list): The list of devices to be considered for data parallelization. cuda (str): The GPU to be considered for loading data. l2 (float,default = 0): The l2 regularization coefficient.","title":"Generic Hyperparameter Tuning parameters"},{"location":"training/#alexnet-hyparameter-tuning","text":"These are hyperparameters for AlexNet: channels (int/bool): If False, the hyperparameter tuning is considered for channels. channels_1 (list): The list containing all values to be tested for channel 1. channels_2 (list): The list containing all values to be tested for channel 2. channels_3 (list): The list containing all values to be tested for channel 3. channels_4 (list): The list containing all values to be tested for channel 4. channels_5 (list): The list containing all values to be tested for channel 5. strides (bool): If False, the hyperparameter tuning is considered for strides. strides_1 (list): The list containing all values to be tested for strides 1. strides_2 (list): The list containing all values to be tested for strides 2. strides_3 (list): The list containing all values to be tested for strides 3. strides_4 (list): The list containing all values to be tested for strides 4. strides_5 (list): The list containing all values to be tested for strides 4. kernel_size (bool): If False, the hyperparameter tuning is considered for kernel size. kernel_size_1 (list): The list containing all values to be tested for kernel size 1. kernel_size_2 (list): The list containing all values to be tested for kernel size 2. kernel_size_3 (list): The list containing all values to be tested for kernel size 3. kernel_size_4 (list): The list containing all values to be tested for kernel size 4. kernel_size_5 (list): The list containing all values to be tested for kernel size 5. padding (bool): If False, the hyperparameter tuning is considered for padding. padding_1 (list): The list containing all values to be tested for padding 1. padding_2 (list): The list containing all values to be tested for padding 2. padding_3 (list): The list containing all values to be tested for padding 3. padding_4 (list): The list containing all values to be tested for padding 4. padding_5 (list): The list containing all values to be tested for padding 5.","title":"AlexNet Hyparameter Tuning"},{"location":"training/#resnet-hyperparameter-tuning","text":"These are hyperparameters for ResNet: groups (bool): If True, the hyperparameter tuning is considered for groups. groups_min (int): The minimum group value to be used for hyperparameter tuning. groups_max (int): The maximum group value to be used for hyperparameter tuning. block (bool): If True, the type of block in resent is considered for hyperparameter. block_list (list): The list containing different type of blocks to be considered for hyperparameter tuning. norm_layer (bool): If True, the type of normalization layers in resent is considered for hyperparameter. norm_layer_list (list): The list containing different type of normalization layers to be considered for hyperparameter tuning. width_per_group (bool): If True, the number of groups per each layer in resent is considered for hyperparameter. width_per_group_list (list): The list containing different types of groups.","title":"ResNet Hyperparameter Tuning"},{"location":"training/#shufflenet-hyperparameter-tuning","text":"These are hyperparameters for ShuffleNet: groups (bool): If True, the number of groups per each layer in resent is considered for hyperparameter. groups_min (int): The minimum group value to be used for hyperparameter tuning. groups_max (int): The maximum group value to be used for hyperparameter tuning. stage_repeat (bool): If True, the number of stage_repeats in shufflenet is considered for hyperparameter. stage_repeat_1 (list): The list containing all the values stage_repeats_1. stage_repeat_2 (list): The list containing all the values stage_repeats_2. stage_repeat_3 (list): The list containing all the values stage_repeats_3.","title":"ShuffleNet Hyperparameter Tuning"},{"location":"training/#xnor-net-hyperparameter-tuning","text":"channels (int/bool): If False, the hyperparameter tuning is considered for channels. channels_1 (list): The list containing all values to be tested for channel 1. channels_2 (list): The list containing all values to be tested for channel 2. channels_3 (list): The list containing all values to be tested for channel 3. channels_4 (list): The list containing all values to be tested for channel 4. channels_5 (list): The list containing all values to be tested for channel 5. strides (bool): If False, the hyperparameter tuning is considered for strides. strides_1 (list): The list containing all values to be tested for strides 1. strides_2 (list): The list containing all values to be tested for strides 2. strides_3 (list): The list containing all values to be tested for strides 3. strides_4 (list): The list containing all values to be tested for strides 4. strides_5 (list): The list containing all values to be tested for strides 4. kernel_size (bool): If False, the hyperparameter tuning is considered for kernel size. kernel_size_1 (list): The list containing all values to be tested for kernel size 1. kernel_size_2 (list): The list containing all values to be tested for kernel size 2. kernel_size_3 (list): The list containing all values to be tested for kernel size 3. kernel_size_4 (list): The list containing all values to be tested for kernel size 4. kernel_size_5 (list): The list containing all values to be tested for kernel size 5. padding (bool): If False, the hyperparameter tuning is considered for padding. padding_1 (list): The list containing all values to be tested for padding 1. padding_2 (list): The list containing all values to be tested for padding 2. padding_3 (list): The list containing all values to be tested for padding 3. padding_4 (list): The list containing all values to be tested for padding 4. padding_5 (list): The list containing all values to be tested for padding 5. groups (bool): If False, the hyperparameter tuning is considered for groups. groups_1 (list): The list containing all values to be tested for groups 1. groups_2 (list): The list containing all values to be tested for groups 2. groups_3 (list): The list containing all values to be tested for groups 3. groups_4 (list): The list containing all values to be tested for groups 4. groups_5 (list): The list containing all values to be tested for groups 5.","title":"XNOR NET Hyperparameter Tuning"},{"location":"transformation/","text":"Transformations Modules Add Noise The Add_Noise is often used for data augmentation and sometimes as regularization. The function adds gaussian centered around the given mean and standard deviation to the input. Parameters: inputs (Torch.Tensor,required): The input tensor which has to be transformed. mean (Torch.Tensor,default = None): The mean of gaussian noise to be added as a tensor. std (Torch.Tensor,default = None): The std of gaussian noise to be added as a tensor. Usage: import torch input = torch.ones(64,512,32,32,32) from NiftyTorch.Transformations.Transformations import Add_Noise output = Add_Noise(input,mean = torch.zeros(input.shape),std = torch.eye(input.shape)) Rotate 90 Rotate the input tensor by 90. Parameters: inputs (torch.Tensor,required): The input tensor which needs to rotated. Usage: import torch input = torch.ones(64,512,32,32,32) from NiftyTorch.Transformations.Transformations import Rotate_90 output = Rotate_90(input) Rotate 180 Rotate the input tensor by 180. Parameters: inputs (torch.Tensor,required): The input tensor which needs to rotated. Usage: import torch input = torch.ones(64,512,32,32,32) from Transformations.Transformations import Rotate_180 output = Rotate_180(input) Rotate 270 Rotate the input tensor by 270. Parameters: inputs (torch.Tensor,required): The input tensor which needs to rotated. Usage: import torch input = torch.ones(64,512,32,32,32) from NiftyTorch.Transformations.Transformations import Rotate_270 output = Rotate_180(input) Random_Segmentation_Crop The Random_Segmentation_Crop is often used with segmentation. The idea behind random crop is select regions around lesions and use only this region to train the network. Parameters: input: the input 3D image. mask: the mask of the lesion region. context: the region around the lesion to be considered for crop. Usage: import torch input = torch.ones(64,512,32,32,32) mask = torch.zeros(64,512,32,32,32) context = 32 from NiftyTorch.Transformations.Transformations import Random_Segmentation_Crop input,mask = Random_Segmentation_Crop(input,mask,context) Resize The Resize module is used to resize the input tensor to the given size. Parameters: input (torch.Tensor,required): the input 3D image. mask (torch.Tensor,default = None): the mask of the lesion region. common: the size to which the tensor is to be resized. Usage: import torch input = torch.ones(64,512,32,32,32) mask = torch.zeros(64,512,32,32,32) context = 32 from NiftyTorch.Transformations.Transformations import Resize input,mask = Resize(input,mask,context)","title":"Transformations"},{"location":"transformation/#transformations","text":"","title":"Transformations"},{"location":"transformation/#modules","text":"","title":"Modules"},{"location":"transformation/#add-noise","text":"The Add_Noise is often used for data augmentation and sometimes as regularization. The function adds gaussian centered around the given mean and standard deviation to the input. Parameters: inputs (Torch.Tensor,required): The input tensor which has to be transformed. mean (Torch.Tensor,default = None): The mean of gaussian noise to be added as a tensor. std (Torch.Tensor,default = None): The std of gaussian noise to be added as a tensor. Usage: import torch input = torch.ones(64,512,32,32,32) from NiftyTorch.Transformations.Transformations import Add_Noise output = Add_Noise(input,mean = torch.zeros(input.shape),std = torch.eye(input.shape))","title":"Add Noise"},{"location":"transformation/#rotate-90","text":"Rotate the input tensor by 90. Parameters: inputs (torch.Tensor,required): The input tensor which needs to rotated. Usage: import torch input = torch.ones(64,512,32,32,32) from NiftyTorch.Transformations.Transformations import Rotate_90 output = Rotate_90(input)","title":"Rotate 90"},{"location":"transformation/#rotate-180","text":"Rotate the input tensor by 180. Parameters: inputs (torch.Tensor,required): The input tensor which needs to rotated. Usage: import torch input = torch.ones(64,512,32,32,32) from Transformations.Transformations import Rotate_180 output = Rotate_180(input)","title":"Rotate 180"},{"location":"transformation/#rotate-270","text":"Rotate the input tensor by 270. Parameters: inputs (torch.Tensor,required): The input tensor which needs to rotated. Usage: import torch input = torch.ones(64,512,32,32,32) from NiftyTorch.Transformations.Transformations import Rotate_270 output = Rotate_180(input)","title":"Rotate 270"},{"location":"transformation/#random_segmentation_crop","text":"The Random_Segmentation_Crop is often used with segmentation. The idea behind random crop is select regions around lesions and use only this region to train the network. Parameters: input: the input 3D image. mask: the mask of the lesion region. context: the region around the lesion to be considered for crop. Usage: import torch input = torch.ones(64,512,32,32,32) mask = torch.zeros(64,512,32,32,32) context = 32 from NiftyTorch.Transformations.Transformations import Random_Segmentation_Crop input,mask = Random_Segmentation_Crop(input,mask,context)","title":"Random_Segmentation_Crop"},{"location":"transformation/#resize","text":"The Resize module is used to resize the input tensor to the given size. Parameters: input (torch.Tensor,required): the input 3D image. mask (torch.Tensor,default = None): the mask of the lesion region. common: the size to which the tensor is to be resized. Usage: import torch input = torch.ones(64,512,32,32,32) mask = torch.zeros(64,512,32,32,32) context = 32 from NiftyTorch.Transformations.Transformations import Resize input,mask = Resize(input,mask,context)","title":"Resize"},{"location":"transformations/","text":"Transformations Modules Add Noise The Add_Noise is often used for data augmentation and sometimes as regularization. The function adds gaussian centered around the given mean and standard deviation to the input. Parameters: inputs (Torch.Tensor,required): The input tensor which has to be transformed. mean (Torch.Tensor,default = None): The mean of gaussian noise to be added as a tensor. std (Torch.Tensor,default = None): The std of gaussian noise to be added as a tensor. Usage: import torch input = torch.ones(64,512,32,32,32) from NiftyTorch.Transformations.Transformations import Add_Noise output = Add_Noise(input,mean = torch.zeros(input.shape),std = torch.eye(input.shape)) Rotate 90 Rotate the input tensor by 90. Parameters: inputs (torch.Tensor,required): The input tensor which needs to rotated. Usage: import torch input = torch.ones(64,512,32,32,32) from NiftyTorch.Transformations.Transformations import Rotate_90 output = Rotate_90(input) Rotate 180 Rotate the input tensor by 180. Parameters: inputs (torch.Tensor,required): The input tensor which needs to rotated. Usage: import torch input = torch.ones(64,512,32,32,32) from Transformations.Transformations import Rotate_180 output = Rotate_180(input) Rotate 270 Rotate the input tensor by 270. Parameters: inputs (torch.Tensor,required): The input tensor which needs to rotated. Usage: import torch input = torch.ones(64,512,32,32,32) from NiftyTorch.Transformations.Transformations import Rotate_270 output = Rotate_180(input) Random_Segmentation_Crop The Random_Segmentation_Crop is often used with segmentation. The idea behind random crop is select regions around lesions and use only this region to train the network. Parameters: input: the input 3D image. mask: the mask of the lesion region. context: the region around the lesion to be considered for crop. Usage: import torch input = torch.ones(64,512,32,32,32) mask = torch.zeros(64,512,32,32,32) context = 32 from NiftyTorch.Transformations.Transformations import Random_Segmentation_Crop input,mask = Random_Segmentation_Crop(input,mask,context) Resize The Resize module is used to resize the input tensor to the given size. Parameters: input (torch.Tensor,required): the input 3D image. mask (torch.Tensor,default = None): the mask of the lesion region. common: the size to which the tensor is to be resized. Usage: import torch input = torch.ones(64,512,32,32,32) mask = torch.zeros(64,512,32,32,32) context = 32 from NiftyTorch.Transformations.Transformations import Resize input,mask = Resize(input,mask,context)","title":"Transformations"},{"location":"transformations/#transformations","text":"","title":"Transformations"},{"location":"transformations/#modules","text":"","title":"Modules"},{"location":"transformations/#add-noise","text":"The Add_Noise is often used for data augmentation and sometimes as regularization. The function adds gaussian centered around the given mean and standard deviation to the input. Parameters: inputs (Torch.Tensor,required): The input tensor which has to be transformed. mean (Torch.Tensor,default = None): The mean of gaussian noise to be added as a tensor. std (Torch.Tensor,default = None): The std of gaussian noise to be added as a tensor. Usage: import torch input = torch.ones(64,512,32,32,32) from NiftyTorch.Transformations.Transformations import Add_Noise output = Add_Noise(input,mean = torch.zeros(input.shape),std = torch.eye(input.shape))","title":"Add Noise"},{"location":"transformations/#rotate-90","text":"Rotate the input tensor by 90. Parameters: inputs (torch.Tensor,required): The input tensor which needs to rotated. Usage: import torch input = torch.ones(64,512,32,32,32) from NiftyTorch.Transformations.Transformations import Rotate_90 output = Rotate_90(input)","title":"Rotate 90"},{"location":"transformations/#rotate-180","text":"Rotate the input tensor by 180. Parameters: inputs (torch.Tensor,required): The input tensor which needs to rotated. Usage: import torch input = torch.ones(64,512,32,32,32) from Transformations.Transformations import Rotate_180 output = Rotate_180(input)","title":"Rotate 180"},{"location":"transformations/#rotate-270","text":"Rotate the input tensor by 270. Parameters: inputs (torch.Tensor,required): The input tensor which needs to rotated. Usage: import torch input = torch.ones(64,512,32,32,32) from NiftyTorch.Transformations.Transformations import Rotate_270 output = Rotate_180(input)","title":"Rotate 270"},{"location":"transformations/#random_segmentation_crop","text":"The Random_Segmentation_Crop is often used with segmentation. The idea behind random crop is select regions around lesions and use only this region to train the network. Parameters: input: the input 3D image. mask: the mask of the lesion region. context: the region around the lesion to be considered for crop. Usage: import torch input = torch.ones(64,512,32,32,32) mask = torch.zeros(64,512,32,32,32) context = 32 from NiftyTorch.Transformations.Transformations import Random_Segmentation_Crop input,mask = Random_Segmentation_Crop(input,mask,context)","title":"Random_Segmentation_Crop"},{"location":"transformations/#resize","text":"The Resize module is used to resize the input tensor to the given size. Parameters: input (torch.Tensor,required): the input 3D image. mask (torch.Tensor,default = None): the mask of the lesion region. common: the size to which the tensor is to be resized. Usage: import torch input = torch.ones(64,512,32,32,32) mask = torch.zeros(64,512,32,32,32) context = 32 from NiftyTorch.Transformations.Transformations import Resize input,mask = Resize(input,mask,context)","title":"Resize"}]}