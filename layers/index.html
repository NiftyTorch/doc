<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Layers - NiftyTorch Documentation</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "Layers";
    var mkdocs_page_input_path = "layers.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> NiftyTorch Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="..">NiftyTorch</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../attention/">Attention</a>
                    </li>
                </ul>
                <ul class="current">
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">Layers</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#modules">Modules</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#conv3x3">Conv3x3</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#conv1x1">Conv1x1</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#basic-block">Basic Block</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#bottleneck-block">BottleNeck Block</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#shuffleunit">ShuffleUnit</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#fire-module">Fire Module</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#binactive">BinActive</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#binconv3d">BinConv3d</a>
    </li>
        </ul>
    </li>
    </ul>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../loader/">Loader</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../loss/">Loss</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../models/">Models</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../training/">Training</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../transformations/">Transformations</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../license/">Licence</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../test.html">Demo</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">NiftyTorch Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Layers</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="layers">Layers</h1>
<p>The Layers module contains set of convolutional neural network units, which can used to create custom neural network architectures.</p>
<h2 id="modules">Modules</h2>
<h3 id="conv3x3">Conv3x3</h3>
<p>This is a simple conv 3x3 layer.</p>
<p>Parameters:
<ul>
<li>in_planes (int,required): The number of channels in the input feature map.
<li>out_planes (int,requied): The number of channels in the output feature map.
<li>stride (int, default = 1): The stride used in each convolution filter.
<li>groups (int, default = 1): The number of groups to be considered while performing convolution. 
<li>dilation (int, default = 1): The field of view to be considered while performing convolution.
<li>bias (int, default = False)
</ul></p>
<p>Usage:</p>
<pre><code class="python">from Layers.Convolutional_Layers import Conv3x3
import torch
in_planes = 512
input = torch.rand(32,in_planes,32,32,32)
convolution = Conv3x3(in_planes = in_planes,out_planes = 32,stride = 1,groups = 1,dilation = 1,bias = False)
output = convolution(input)
</code></pre>

<h3 id="conv1x1">Conv1x1</h3>
<p>This is a simple conv 1x1 layer.</p>
<p>Parameters:
<ul>
<li>in_planes (int,required): The number of channels in the input feature map.
<li>out_planes (int,requied): The number of channels in the output feature map.
<li>stride (int, default = 1): The stride used in each convolution filter.
<li>groups (int, default = 1): The number of groups to be considered while performing convolution.
<li>bias (int, default = False)
</ul></p>
<p>Usage:</p>
<pre><code class="python">from NiftyTorch.Layers.Convolutional_Layers import Conv1x1
import torch
in_planes = 512
input = torch.rand(32,in_planes,32,32,32)
convolution = Conv1x1(in_planes = in_planes,out_planes = 32,stride = 1,groups = 1,bias = False)
output = convolution(input)
</code></pre>

<h3 id="basic-block">Basic Block</h3>
<p><code>BasicBlock</code> is a series of convolution, batchnorm, activation layers with one jump skip connection. </p>
<p>The structure of this block is as follows:</p>
<p>input -&gt; conv1 -&gt; bn1 -&gt; relu -&gt; conv2 -&gt; bn2 + input -&gt; relu : output</p>
<p>Parameters:</p>
<ul>
<li>inplanes (int,required): The number of channels in the input feature map. 
<li>planes (int,required): The number of channels in the output feature map.
<li>stride (int,default = 1): The number of strides to be used in the conv1
<li>downsample (int,default = None): The downsampler to be used to reduce the feature map size F.interpolate
<li>groups (int,default = 1): The number of groups to be considered in convolution
<li>dilation (int, default = 1): The field of view to be considered while performing convolution.
<li>norm_layer (int,default = None): The normalization layer to be used ex: nn.BatchNorm()
</ul>

<p>Usage:</p>
<pre><code class="python">from NiftyTorch.Layers.Convolutional_Layers import BasicBlock
import torch
in_planes = 512
input = torch.rand(32,in_planes,32,32,32)
convolution_block = BasicBlock(in_planes = in_planes,planes = 256,out_planes = 32,stride = 1,groups = 1,bias = False)
output = convolution_block(input)
</code></pre>

<h3 id="bottleneck-block">BottleNeck Block</h3>
<p><code>BottleNeck</code> is a series of convolution,batchnorm,activation layers with two layer jump skip connection. </p>
<p>The structure of this block is as follows:</p>
<p>input -&gt; conv1 -&gt; bn1 -&gt; relu -&gt; conv2 -&gt; bn2 -&gt; relu -&gt; conv3 -&gt; bn3 + input -&gt; relu : output</p>
<p>Parameters:</p>
<ul>
<li>inplanes (int,required): The number of channels in the input feature map. 
<li>planes (int,required): The number of channels in the output feature map.
<li>stride (int,default = 1): The number of strides to be used in the conv1
<li>downsample (int,default = None): The downsampler to be used to reduce the feature map size F.interpolate
<li>groups (int,default = 1): The number of groups to be considered in convolution.
<li>base_width (int,default = 1): The base width and expansion are used to calculate the number of filters for conv2 in bottleneck block.
<li>dilation (int, default = 1): The field of view to be considered while performing convolution.
<li>norm_layer (int,default = None): The normalization layer to be used ex: nn.BatchNorm()
<li>expansion (int,default = 1):The base width and expansion are used to calculate the number of filters for conv2 in bottleneck block.
</ul>

<p>Usage:</p>
<pre><code class="python">from NiftyTorch.Layers.Convolutional_Layers import BottleNeck
import torch
in_planes = 512
planes = 256
input = torch.rand(32,in_planes,32,32,32)
convolution_block = BottleNeck(inplanes = in_planes, planes = planes, stride=1, downsample=None, groups=1,base_width=64, dilation=1, norm_layer=None,expansion = 1)
output = convolution_block(input)
</code></pre>

<h3 id="shuffleunit">ShuffleUnit</h3>
<p><code>ShuffleUnit</code> is the class definition for ShuffleUnit Module, which uses channel shuffling and combination to improve the network performance with fewer parameters.<br>
The ShuffleUnit is commonly used in ShuffleNet 1.0 and ShuffleNet 2.0, but they can be used along with any network. For more information, please read the ShuffleNet papers [<a href="https://arxiv.org/abs/1707.01083">ShuffleNet 1.0</a>, <a href="https://arxiv.org/abs/1807.11164">ShuffleNet 2.0</a>].<br></p>
<p>Parameters:</p>
<ul>
<li>in_channels (int,required): number of  channels in the input tensor the shuffleunit. 
<li>out_channels (int,required): number of channels in the tensor generated from the shuffleunit.
<li>groups (int,default = 3): define number of groups in which the filters are combined before shuffle operation.
<li>grouped_conv (bool,default = True): defines whether grouped_convolution is to be used or not.
<li>combine ('add' or 'concat',default = 'add'): defines the method in which we can combine the channels it has two options add or concat.
<li>compresstion_ratio (int,default = 4): The number of channels to be required in the bottleneck channels.
</ul>

<p>Usage:</p>
<pre><code class="python">from NiftyTorch.Layers.Convolutional_Layers import ShuffleUnit
import torch
in_planes = 512
planes = 256
input = torch.rand(32,in_planes,32,32,32)
convolution_block = ShuffleUnit(inplanes = in_planes, planes = planes, groups=5, grouped_conv=True, combine = 'concat',compression_ratio = 4)
output = convolution_block(input)
</code></pre>

<h3 id="fire-module">Fire Module</h3>
<p><code>Fire</code> is the class definition for Fire Module, which uses a combination of 1x1 and 3x3 filters to improve the network performance with fewer parameters.<br>
The Fire Module is commonly used in Squeezenet 1.0 and SqueezeNet 2.0 but they can used along with any network. For more information please read the <a href="https://arxiv.org/abs/1602.07360">SqueezeNet paper</a>.<br></p>
<p>Parameters:</p>
<ul>
<li>in_channels (int,required): number of  channels in the input tensor the shuffleunit. 
<li>out_channels (int,required): number of channels in the tensor generated from the shuffleunit.
<li>groups (int,default = 3): define number of groups in which the filters are combined before shuffle operation.
<li>grouped_conv (bool,default = True): defines whether grouped_convolution is to be used or not.
<li>combine ('add' or 'concat',default = 'add'): defines the method in which we can combine the channels it has two options add or concat.
<li>compresstion_ratio (int,default = 4): The number of channels to be required in the bottleneck channels.
</ul>

<p>Usage:</p>
<pre><code class="python">from NiftyTorch.Layers.Convolutional_Layers import Fire
import torch
in_planes = 20
input = torch.rand(32,20,32,32,32)
convolution_block = Fire(inplanes = in_planes, squeeze_planes = 3, expand1x1_planes = 12, expand3x3_planes = 12)
output = convolution_block(input)
</code></pre>

<h3 id="binactive">BinActive</h3>
<p>The <code>BinActive</code> class is for calling binary activation method, which gives the sign of the input tensor as an activation.<br>
This is used in Binary Networks and XNOR Network.<br></p>
<p>Usage:</p>
<pre><code class="python">import torch
from NiftyTorch.Layers.Convolutional_Layers import BinActive
input = torch.ones(32,512,32,32,32)
activation = BinActive()
output = activation(input)
</code></pre>

<h3 id="binconv3d">BinConv3d</h3>
<p>Parameters:</p>
<ul>
<li>input_channels (int,required): The number of channels in the input tensor.
<li>output_channels (int,required): The number of channels in the output tensor.
<li>kernel_size (int,default = 3): Kernel size for the convolution filter.
<li>stride (int,default = 1): The number of strides in the convolutional filters.
<li>padding (int,default = 0): The padding which is to be done on the ends.
<li>groups (int,default = 1): Number of groups in the convolutional filters.
<li>dropout (int,default = 0): The dropout probability.
<li>Linear (bool,default = False): If True, instead of convolution.
</ul>

<p>Usage:</p>
<pre><code class="python">import torch
from NiftyTorch.Layers.Convolutional_Layers import BinConv3d
in_channels = 512
input = torch.ones(32,in_channels,32,32,32)
activation = BinConv3d(input_channels = in_channels,output_channels = 256,kernel_size = 3,stride = 2,padding = 1,groups = 1,dropout = 0.5,Linear = False)
output = activation(input)
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../loader/" class="btn btn-neutral float-right" title="Loader">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../attention/" class="btn btn-neutral" title="Attention"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../attention/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../loader/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
